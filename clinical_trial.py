import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import scipy.stats as stats
from scipy.stats import chi2_contingency, fisher_exact, mannwhitneyu, ttest_ind, wilcoxon
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

def clinical_trial_analysis():
    """ä¸´åºŠè¯•éªŒåˆ†æä¸»å‡½æ•°"""
    st.markdown("# ğŸ§ª ä¸´åºŠè¯•éªŒåˆ†æ")
    st.markdown("*ä¸“ä¸šçš„ä¸´åºŠè¯•éªŒæ•°æ®åˆ†æå·¥å…·ï¼Œæ”¯æŒå¤šç§è¯•éªŒè®¾è®¡å’Œç»Ÿè®¡åˆ†æ*")
    
    # ä¾§è¾¹æ å¯¼èˆª
    with st.sidebar:
        st.markdown("## ğŸ§ª åˆ†ææ¨¡å—")
        analysis_type = st.radio(
            "é€‰æ‹©åˆ†æç±»å‹",
            [
                "ğŸ“Š åŸºçº¿ç‰¹å¾åˆ†æ",
                "ğŸ¯ ä¸»è¦ç»ˆç‚¹åˆ†æ", 
                "ğŸ“ˆ æ¬¡è¦ç»ˆç‚¹åˆ†æ",
                "ğŸ›¡ï¸ å®‰å…¨æ€§åˆ†æ",
                "ğŸ“‹ äºšç»„åˆ†æ",
                "â±ï¸ æ—¶é—´è¶‹åŠ¿åˆ†æ",
                "ğŸ” æ•æ„Ÿæ€§åˆ†æ",
                "ğŸ“„ è¯•éªŒæ€»ç»“æŠ¥å‘Š"
            ]
        )
    
    # æ£€æŸ¥æ•°æ®
    datasets = get_available_datasets()
    if not datasets:
        st.warning("âš ï¸ è¯·å…ˆåœ¨æ•°æ®ç®¡ç†æ¨¡å—ä¸­å¯¼å…¥ä¸´åºŠè¯•éªŒæ•°æ®")
        st.info("ğŸ’¡ æ‚¨å¯ä»¥ä½¿ç”¨ç¤ºä¾‹æ•°æ®é›†ä¸­çš„'ä¸´åºŠè¯•éªŒæ•°æ®'è¿›è¡Œå­¦ä¹ ")
        
        # æä¾›ç¤ºä¾‹æ•°æ®é€‰é¡¹
        if st.button("ğŸ² ç”Ÿæˆä¸´åºŠè¯•éªŒç¤ºä¾‹æ•°æ®", use_container_width=True):
            sample_data = generate_clinical_trial_sample_data()
            st.session_state['dataset_clinical_sample'] = {
                'name': 'ä¸´åºŠè¯•éªŒç¤ºä¾‹æ•°æ®',
                'data': sample_data,
                'upload_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            st.success("âœ… ç¤ºä¾‹æ•°æ®å·²ç”Ÿæˆï¼")
            st.rerun()
        return
    
    # é€‰æ‹©æ•°æ®é›†
    selected_dataset = st.selectbox(
        "ğŸ“Š é€‰æ‹©ä¸´åºŠè¯•éªŒæ•°æ®é›†", 
        list(datasets.keys()),
        help="é€‰æ‹©åŒ…å«ä¸´åºŠè¯•éªŒæ•°æ®çš„æ•°æ®é›†"
    )
    df = datasets[selected_dataset]['data']
    
    # æ•°æ®éªŒè¯
    if not validate_clinical_data(df):
        return
    
    # æ ¹æ®é€‰æ‹©çš„åˆ†æç±»å‹è°ƒç”¨ç›¸åº”å‡½æ•°
    if analysis_type == "ğŸ“Š åŸºçº¿ç‰¹å¾åˆ†æ":
        baseline_characteristics_analysis(df)
    elif analysis_type == "ğŸ¯ ä¸»è¦ç»ˆç‚¹åˆ†æ":
        primary_endpoint_analysis(df)
    elif analysis_type == "ğŸ“ˆ æ¬¡è¦ç»ˆç‚¹åˆ†æ":
        secondary_endpoint_analysis(df)
    elif analysis_type == "ğŸ›¡ï¸ å®‰å…¨æ€§åˆ†æ":
        safety_analysis(df)
    elif analysis_type == "ğŸ“‹ äºšç»„åˆ†æ":
        subgroup_analysis(df)
    elif analysis_type == "â±ï¸ æ—¶é—´è¶‹åŠ¿åˆ†æ":
        time_trend_analysis(df)
    elif analysis_type == "ğŸ” æ•æ„Ÿæ€§åˆ†æ":
        sensitivity_analysis(df)
    elif analysis_type == "ğŸ“„ è¯•éªŒæ€»ç»“æŠ¥å‘Š":
        trial_summary_report(df)

def get_available_datasets():
    """è·å–å¯ç”¨çš„æ•°æ®é›†"""
    datasets = {}
    for key, value in st.session_state.items():
        if key.startswith('dataset_') and isinstance(value, dict) and 'data' in value:
            datasets[value.get('name', key)] = value
    return datasets

def validate_clinical_data(df):
    """éªŒè¯ä¸´åºŠè¯•éªŒæ•°æ®æ ¼å¼"""
    st.markdown("### ğŸ“‹ æ•°æ®éªŒè¯")
    
    required_cols = ['æ²»ç–—ç»„', 'å—è¯•è€…ID']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        st.error(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
        st.info("ğŸ’¡ ä¸´åºŠè¯•éªŒæ•°æ®åº”åŒ…å«: æ²»ç–—ç»„ã€å—è¯•è€…IDç­‰åŸºæœ¬ä¿¡æ¯")
        
        # æä¾›åˆ—æ˜ å°„é€‰é¡¹
        with st.expander("ğŸ”§ åˆ—åæ˜ å°„", expanded=True):
            st.markdown("è¯·å°†æ‚¨çš„æ•°æ®åˆ—æ˜ å°„åˆ°æ ‡å‡†æ ¼å¼:")
            
            col1, col2 = st.columns(2)
            with col1:
                if 'æ²»ç–—ç»„' not in df.columns:
                    treatment_col = st.selectbox("æ²»ç–—ç»„åˆ—", [''] + df.columns.tolist())
                    if treatment_col:
                        df['æ²»ç–—ç»„'] = df[treatment_col]
            
            with col2:
                if 'å—è¯•è€…ID' not in df.columns:
                    subject_col = st.selectbox("å—è¯•è€…IDåˆ—", [''] + df.columns.tolist())
                    if subject_col:
                        df['å—è¯•è€…ID'] = df[subject_col]
            
            if st.button("âœ… åº”ç”¨æ˜ å°„"):
                st.success("æ˜ å°„å·²åº”ç”¨ï¼Œè¯·é‡æ–°è¿è¡Œåˆ†æ")
                st.rerun()
        
        return False
    
    # æ•°æ®è´¨é‡æ£€æŸ¥
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ“Š æ€»å—è¯•è€…æ•°", len(df))
    with col2:
        treatment_groups = df['æ²»ç–—ç»„'].nunique()
        st.metric("ğŸ¯ æ²»ç–—ç»„æ•°", treatment_groups)
    with col3:
        missing_rate = df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100
        st.metric("âŒ ç¼ºå¤±ç‡", f"{missing_rate:.1f}%")
    with col4:
        duplicate_subjects = df['å—è¯•è€…ID'].duplicated().sum()
        st.metric("ğŸ”„ é‡å¤å—è¯•è€…", duplicate_subjects)
    
    # æ²»ç–—ç»„åˆ†å¸ƒ
    treatment_dist = df['æ²»ç–—ç»„'].value_counts()
    st.markdown("**æ²»ç–—ç»„åˆ†å¸ƒ:**")
    
    col1, col2 = st.columns([1, 2])
    with col1:
        st.dataframe(treatment_dist.reset_index().rename(columns={'index': 'æ²»ç–—ç»„', 'æ²»ç–—ç»„': 'äººæ•°'}))
    with col2:
        fig = px.pie(values=treatment_dist.values, names=treatment_dist.index, 
                     title="æ²»ç–—ç»„åˆ†å¸ƒ", color_discrete_sequence=px.colors.qualitative.Set3)
        st.plotly_chart(fig, use_container_width=True)
    
    return True

def baseline_characteristics_analysis(df):
    """åŸºçº¿ç‰¹å¾åˆ†æ"""
    st.markdown("### ğŸ“Š åŸºçº¿ç‰¹å¾åˆ†æ")
    st.markdown("*æ¯”è¾ƒå„æ²»ç–—ç»„é—´åŸºçº¿ç‰¹å¾çš„å¹³è¡¡æ€§*")
    
    # è¯†åˆ«åŸºçº¿å˜é‡
    baseline_vars = identify_baseline_variables(df)
    
    if not baseline_vars:
        st.warning("âš ï¸ æœªè¯†åˆ«åˆ°åŸºçº¿å˜é‡")
        return
    
    # é€‰æ‹©è¦åˆ†æçš„åŸºçº¿å˜é‡
    selected_vars = st.multiselect(
        "é€‰æ‹©åŸºçº¿å˜é‡",
        baseline_vars,
        default=baseline_vars[:10] if len(baseline_vars) >= 10 else baseline_vars,
        help="é€‰æ‹©è¦è¿›è¡Œç»„é—´æ¯”è¾ƒçš„åŸºçº¿å˜é‡"
    )
    
    if not selected_vars:
        return
    
    # åˆ†æé€‰é¡¹
    col1, col2, col3 = st.columns(3)
    with col1:
        show_pvalues = st.checkbox("æ˜¾ç¤ºPå€¼", value=True)
    with col2:
        alpha_level = st.selectbox("æ˜¾è‘—æ€§æ°´å¹³", [0.05, 0.01, 0.001], index=0)
    with col3:
        effect_size = st.checkbox("è®¡ç®—æ•ˆåº”é‡", value=True)
    
    # æ‰§è¡ŒåŸºçº¿ç‰¹å¾åˆ†æ
    results = perform_baseline_analysis(df, selected_vars, show_pvalues, alpha_level, effect_size)
    
    # æ˜¾ç¤ºç»“æœè¡¨æ ¼
    st.markdown("#### ğŸ“‹ åŸºçº¿ç‰¹å¾æ¯”è¾ƒè¡¨")
    
    # æ ¼å¼åŒ–ç»“æœè¡¨æ ¼
    formatted_results = format_baseline_table(results, show_pvalues, effect_size)
    st.dataframe(formatted_results, use_container_width=True)
    
    # å¯è§†åŒ–åŸºçº¿ç‰¹å¾
    st.markdown("#### ğŸ“Š åŸºçº¿ç‰¹å¾å¯è§†åŒ–")
    
    # é€‰æ‹©å¯è§†åŒ–å˜é‡
    viz_var = st.selectbox("é€‰æ‹©å¯è§†åŒ–å˜é‡", selected_vars)
    
    if viz_var:
        create_baseline_visualization(df, viz_var)
    
    # åŸºçº¿ä¸å¹³è¡¡æ£€æµ‹
    st.markdown("#### âš–ï¸ åŸºçº¿å¹³è¡¡æ€§è¯„ä¼°")
    
    imbalanced_vars = detect_baseline_imbalance(results, alpha_level)
    
    if imbalanced_vars:
        st.warning(f"âš ï¸ å‘ç° {len(imbalanced_vars)} ä¸ªåŸºçº¿ä¸å¹³è¡¡å˜é‡:")
        for var in imbalanced_vars:
            st.write(f"â€¢ {var}")
        
        st.info("ğŸ’¡ å»ºè®®åœ¨ä¸»è¦åˆ†æä¸­è€ƒè™‘è¿™äº›å˜é‡ä½œä¸ºåå˜é‡è¿›è¡Œè°ƒæ•´")
    else:
        st.success("âœ… æ‰€æœ‰åŸºçº¿å˜é‡åœ¨ç»„é—´å‡è¡¡è‰¯å¥½")
    
    # å¯¼å‡ºåŸºçº¿ç‰¹å¾è¡¨
    if st.button("ğŸ“¥ å¯¼å‡ºåŸºçº¿ç‰¹å¾è¡¨"):
        export_baseline_table(formatted_results)

def identify_baseline_variables(df):
    """è¯†åˆ«åŸºçº¿å˜é‡"""
    # å¸¸è§çš„åŸºçº¿å˜é‡å…³é”®è¯
    baseline_keywords = [
        'å¹´é¾„', 'æ€§åˆ«', 'ä½“é‡', 'èº«é«˜', 'BMI', 'è¡€å‹', 'åŸºçº¿', 
        'å…¥ç»„', 'ç­›é€‰', 'äººå£å­¦', 'æ—¢å¾€å²', 'åˆå¹¶ç”¨è¯', 'ç—…å²'
    ]
    
    baseline_vars = []
    
    # æ’é™¤æ˜æ˜¾çš„ç»“å±€å˜é‡
    exclude_keywords = [
        'ç»ˆç‚¹', 'ç–—æ•ˆ', 'ä¸è‰¯äº‹ä»¶', 'éšè®¿', 'å‡ºç»„', 'å®Œæˆ', 
        'ä¾ä»æ€§', 'æ»¡æ„åº¦', 'è¯„ä¼°', 'æ”¹å–„'
    ]
    
    for col in df.columns:
        if col in ['å—è¯•è€…ID', 'æ²»ç–—ç»„']:
            continue
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºçº¿å…³é”®è¯
        is_baseline = any(keyword in col for keyword in baseline_keywords)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ’é™¤å˜é‡
        is_exclude = any(keyword in col for keyword in exclude_keywords)
        
        if is_baseline or (not is_exclude and col not in baseline_vars):
            # è¿›ä¸€æ­¥æ£€æŸ¥æ•°æ®ç±»å‹å’Œåˆ†å¸ƒ
            if df[col].dtype in ['object', 'category'] or df[col].nunique() < len(df) * 0.8:
                baseline_vars.append(col)
    
    return baseline_vars

def perform_baseline_analysis(df, variables, show_pvalues, alpha_level, effect_size):
    """æ‰§è¡ŒåŸºçº¿ç‰¹å¾åˆ†æ"""
    results = []
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    
    for var in variables:
        var_result = {'å˜é‡': var}
        
        # æ£€æŸ¥å˜é‡ç±»å‹
        if df[var].dtype in ['object', 'category'] or df[var].nunique() <= 10:
            # åˆ†ç±»å˜é‡
            var_result.update(analyze_categorical_baseline(df, var, treatment_groups, show_pvalues, alpha_level, effect_size))
        else:
            # è¿ç»­å˜é‡
            var_result.update(analyze_continuous_baseline(df, var, treatment_groups, show_pvalues, alpha_level, effect_size))
        
        results.append(var_result)
    
    return results

def analyze_categorical_baseline(df, var, treatment_groups, show_pvalues, alpha_level, effect_size):
    """åˆ†æåˆ†ç±»åŸºçº¿å˜é‡"""
    result = {'ç±»å‹': 'åˆ†ç±»å˜é‡'}
    
    # è®¡ç®—å„ç»„çš„é¢‘æ•°å’Œç™¾åˆ†æ¯”
    for group in treatment_groups:
        group_data = df[df['æ²»ç–—ç»„'] == group][var]
        value_counts = group_data.value_counts()
        total = len(group_data.dropna())
        
        if total > 0:
            # æ ¼å¼åŒ–ä¸º "n (%)"
            formatted_values = []
            for value, count in value_counts.items():
                pct = count / total * 100
                formatted_values.append(f"{count} ({pct:.1f}%)")
            
            result[f'{group}'] = "; ".join(formatted_values)
        else:
            result[f'{group}'] = "æ— æ•°æ®"
    
    # ç»Ÿè®¡æ£€éªŒ
    if show_pvalues:
        try:
            # åˆ›å»ºåˆ—è”è¡¨
            crosstab = pd.crosstab(df[var], df['æ²»ç–—ç»„'])
            
            if crosstab.shape[0] == 2 and crosstab.shape[1] == 2:
                # 2x2è¡¨ï¼Œä½¿ç”¨Fisherç²¾ç¡®æ£€éªŒ
                _, p_value = fisher_exact(crosstab)
                result['æ£€éªŒæ–¹æ³•'] = "Fisherç²¾ç¡®æ£€éªŒ"
            else:
                # å¡æ–¹æ£€éªŒ
                chi2, p_value, _, _ = chi2_contingency(crosstab)
                result['æ£€éªŒæ–¹æ³•'] = "å¡æ–¹æ£€éªŒ"
            
            result['På€¼'] = f"{p_value:.4f}"
            result['æ˜¾è‘—æ€§'] = "æ˜¯" if p_value < alpha_level else "å¦"
            
            # æ•ˆåº”é‡ (CramÃ©r's V)
            if effect_size:
                n = crosstab.sum().sum()
                cramers_v = np.sqrt(chi2 / (n * (min(crosstab.shape) - 1)))
                result['æ•ˆåº”é‡(CramÃ©r\'s V)'] = f"{cramers_v:.3f}"
                
        except Exception as e:
            result['På€¼'] = "è®¡ç®—å¤±è´¥"
            result['æ£€éªŒæ–¹æ³•'] = "æ— æ³•è®¡ç®—"
    
    return result

def analyze_continuous_baseline(df, var, treatment_groups, show_pvalues, alpha_level, effect_size):
    """åˆ†æè¿ç»­åŸºçº¿å˜é‡"""
    result = {'ç±»å‹': 'è¿ç»­å˜é‡'}
    
    # è®¡ç®—å„ç»„çš„æè¿°æ€§ç»Ÿè®¡
    for group in treatment_groups:
        group_data = df[df['æ²»ç–—ç»„'] == group][var].dropna()
        
        if len(group_data) > 0:
            mean = group_data.mean()
            std = group_data.std()
            median = group_data.median()
            q1 = group_data.quantile(0.25)
            q3 = group_data.quantile(0.75)
            
            # æ ¹æ®æ•°æ®åˆ†å¸ƒé€‰æ‹©æè¿°æ–¹å¼
            if is_normally_distributed(group_data):
                result[f'{group}'] = f"{mean:.2f} Â± {std:.2f}"
            else:
                result[f'{group}'] = f"{median:.2f} ({q1:.2f}, {q3:.2f})"
        else:
            result[f'{group}'] = "æ— æ•°æ®"
    
    # ç»Ÿè®¡æ£€éªŒ
    if show_pvalues and len(treatment_groups) >= 2:
        try:
            group_data_list = []
            for group in treatment_groups:
                group_data = df[df['æ²»ç–—ç»„'] == group][var].dropna()
                group_data_list.append(group_data)
            
            if len(treatment_groups) == 2:
                # ä¸¤ç»„æ¯”è¾ƒ
                group1_data, group2_data = group_data_list[0], group_data_list[1]
                
                # æ­£æ€æ€§æ£€éªŒ
                if (is_normally_distributed(group1_data) and is_normally_distributed(group2_data) 
                    and len(group1_data) >= 30 and len(group2_data) >= 30):
                    # tæ£€éªŒ
                    _, p_value = ttest_ind(group1_data, group2_data)
                    result['æ£€éªŒæ–¹æ³•'] = "ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ"
                else:
                    # Mann-Whitney Uæ£€éªŒ
                    _, p_value = mannwhitneyu(group1_data, group2_data, alternative='two-sided')
                    result['æ£€éªŒæ–¹æ³•'] = "Mann-Whitney Uæ£€éªŒ"
                
                # æ•ˆåº”é‡
                if effect_size:
                    cohens_d = calculate_cohens_d(group1_data, group2_data)
                    result['æ•ˆåº”é‡(Cohen\'s d)'] = f"{cohens_d:.3f}"
            
            else:
                # å¤šç»„æ¯”è¾ƒ
                from scipy.stats import kruskal, f_oneway
                
                # æ£€æŸ¥æ­£æ€æ€§
                all_normal = all(is_normally_distributed(data) for data in group_data_list if len(data) >= 8)
                
                if all_normal:
                    # æ–¹å·®åˆ†æ
                    _, p_value = f_oneway(*group_data_list)
                    result['æ£€éªŒæ–¹æ³•'] = "å•å› ç´ æ–¹å·®åˆ†æ"
                else:
                    # Kruskal-Wallisæ£€éªŒ
                    _, p_value = kruskal(*group_data_list)
                    result['æ£€éªŒæ–¹æ³•'] = "Kruskal-Wallisæ£€éªŒ"
            
            result['På€¼'] = f"{p_value:.4f}"
            result['æ˜¾è‘—æ€§'] = "æ˜¯" if p_value < alpha_level else "å¦"
            
        except Exception as e:
            result['På€¼'] = "è®¡ç®—å¤±è´¥"
            result['æ£€éªŒæ–¹æ³•'] = "æ— æ³•è®¡ç®—"
    
    return result

def is_normally_distributed(data, alpha=0.05):
    """æ£€éªŒæ•°æ®æ˜¯å¦æ­£æ€åˆ†å¸ƒ"""
    if len(data) < 8:
        return False
    
    try:
        from scipy.stats import shapiro, normaltest
        
        if len(data) <= 5000:
            _, p_value = shapiro(data)
        else:
            _, p_value = normaltest(data)
        
        return p_value > alpha
    except:
        return False

def calculate_cohens_d(group1, group2):
    """è®¡ç®—Cohen's dæ•ˆåº”é‡"""
    n1, n2 = len(group1), len(group2)
    
    if n1 == 0 or n2 == 0:
        return 0
    
    # è®¡ç®—åˆå¹¶æ ‡å‡†å·®
    pooled_std = np.sqrt(((n1 - 1) * group1.var() + (n2 - 1) * group2.var()) / (n1 + n2 - 2))
    
    if pooled_std == 0:
        return 0
    
    return (group1.mean() - group2.mean()) / pooled_std

def format_baseline_table(results, show_pvalues, effect_size):
    """æ ¼å¼åŒ–åŸºçº¿ç‰¹å¾è¡¨æ ¼"""
    df_results = pd.DataFrame(results)
    
    # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
    columns_order = ['å˜é‡', 'ç±»å‹']
    
    # æ·»åŠ æ²»ç–—ç»„åˆ—
    treatment_cols = [col for col in df_results.columns if col not in ['å˜é‡', 'ç±»å‹', 'På€¼', 'æ£€éªŒæ–¹æ³•', 'æ˜¾è‘—æ€§'] and not col.startswith('æ•ˆåº”é‡')]
    columns_order.extend(treatment_cols)
    
    if show_pvalues:
        columns_order.extend(['æ£€éªŒæ–¹æ³•', 'På€¼', 'æ˜¾è‘—æ€§'])
    
    if effect_size:
        effect_cols = [col for col in df_results.columns if col.startswith('æ•ˆåº”é‡')]
        columns_order.extend(effect_cols)
    
    # é‡æ–°æ’åˆ—åˆ—
    available_columns = [col for col in columns_order if col in df_results.columns]
    df_results = df_results[available_columns]
    
    return df_results

def create_baseline_visualization(df, var):
    """åˆ›å»ºåŸºçº¿ç‰¹å¾å¯è§†åŒ–"""
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    
    if df[var].dtype in ['object', 'category'] or df[var].nunique() <= 10:
        # åˆ†ç±»å˜é‡ - å †ç§¯æŸ±çŠ¶å›¾
        crosstab = pd.crosstab(df[var], df['æ²»ç–—ç»„'], normalize='columns') * 100
        
        fig = px.bar(
            crosstab.reset_index(),
            x=var,
            y=crosstab.columns.tolist(),
            title=f"{var} åœ¨å„æ²»ç–—ç»„ä¸­çš„åˆ†å¸ƒ",
            labels={'value': 'ç™¾åˆ†æ¯” (%)', 'variable': 'æ²»ç–—ç»„'},
            barmode='group'
        )
        
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
        
    else:
        # è¿ç»­å˜é‡ - ç®±çº¿å›¾å’Œç›´æ–¹å›¾
        col1, col2 = st.columns(2)
        
        with col1:
            # ç®±çº¿å›¾
            fig_box = px.box(
                df, x='æ²»ç–—ç»„', y=var,
                title=f"{var} ç®±çº¿å›¾æ¯”è¾ƒ",
                points="outliers"
            )
            fig_box.update_layout(height=400)
            st.plotly_chart(fig_box, use_container_width=True)
        
        with col2:
            # åˆ†ç»„ç›´æ–¹å›¾
            fig_hist = px.histogram(
                df, x=var, color='æ²»ç–—ç»„',
                title=f"{var} åˆ†å¸ƒç›´æ–¹å›¾",
                barmode='overlay',
                opacity=0.7
            )
            fig_hist.update_layout(height=400)
            st.plotly_chart(fig_hist, use_container_width=True)

def detect_baseline_imbalance(results, alpha_level):
    """æ£€æµ‹åŸºçº¿ä¸å¹³è¡¡å˜é‡"""
    imbalanced_vars = []
    
    for result in results:
        if 'På€¼' in result and result['På€¼'] != "è®¡ç®—å¤±è´¥":
            try:
                p_value = float(result['På€¼'])
                if p_value < alpha_level:
                    imbalanced_vars.append(result['å˜é‡'])
            except:
                continue
    
    return imbalanced_vars

def export_baseline_table(formatted_results):
    """å¯¼å‡ºåŸºçº¿ç‰¹å¾è¡¨"""
    import io
    
    # è½¬æ¢ä¸ºExcelæ ¼å¼
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        formatted_results.to_excel(writer, sheet_name='åŸºçº¿ç‰¹å¾åˆ†æ', index=False)
        
        # æ ¼å¼åŒ–å·¥ä½œè¡¨
        workbook = writer.book
        worksheet = writer.sheets['åŸºçº¿ç‰¹å¾åˆ†æ']
        
        # è®¾ç½®åˆ—å®½
        for i, col in enumerate(formatted_results.columns):
            max_len = max(len(str(col)), formatted_results[col].astype(str).str.len().max())
            worksheet.set_column(i, i, min(max_len + 2, 50))
    
    output.seek(0)
    
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½åŸºçº¿ç‰¹å¾è¡¨",
        data=output.getvalue(),
        file_name=f"åŸºçº¿ç‰¹å¾åˆ†æ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

def primary_endpoint_analysis(df):
    """ä¸»è¦ç»ˆç‚¹åˆ†æ"""
    st.markdown("### ğŸ¯ ä¸»è¦ç»ˆç‚¹åˆ†æ")
    st.markdown("*åˆ†æè¯•éªŒçš„ä¸»è¦ç–—æ•ˆç»ˆç‚¹*")
    
    # è¯†åˆ«å¯èƒ½çš„ä¸»è¦ç»ˆç‚¹å˜é‡
    endpoint_vars = identify_endpoint_variables(df, endpoint_type='primary')
    
    if not endpoint_vars:
        st.warning("âš ï¸ æœªè¯†åˆ«åˆ°ä¸»è¦ç»ˆç‚¹å˜é‡")
        st.info("ğŸ’¡ è¯·ç¡®ä¿æ•°æ®ä¸­åŒ…å«ä¸»è¦ç–—æ•ˆæŒ‡æ ‡")
        return
    
    # é€‰æ‹©ä¸»è¦ç»ˆç‚¹
    col1, col2 = st.columns(2)
    with col1:
        primary_endpoint = st.selectbox(
            "é€‰æ‹©ä¸»è¦ç»ˆç‚¹å˜é‡",
            endpoint_vars,
            help="é€‰æ‹©è¯•éªŒçš„ä¸»è¦ç–—æ•ˆç»ˆç‚¹"
        )
    
    with col2:
        endpoint_type = st.selectbox(
            "ç»ˆç‚¹ç±»å‹",
            ["è¿ç»­å‹", "äºŒåˆ†ç±»", "æ—¶é—´-äº‹ä»¶", "æœ‰åºåˆ†ç±»"],
            help="é€‰æ‹©ç»ˆç‚¹å˜é‡çš„æ•°æ®ç±»å‹"
        )
    
    if not primary_endpoint:
        return
    
    # åˆ†æè®¾ç½®
    with st.expander("ğŸ”§ åˆ†æè®¾ç½®", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            alpha_level = st.selectbox("æ˜¾è‘—æ€§æ°´å¹³", [0.05, 0.01, 0.001], index=0)
            confidence_level = 1 - alpha_level
        
        with col2:
            analysis_method = st.selectbox(
                "åˆ†ææ–¹æ³•",
                ["æ„å‘æ€§æ²»ç–—åˆ†æ(ITT)", "ç¬¦åˆæ–¹æ¡ˆé›†åˆ†æ(PP)", "å®‰å…¨æ€§åˆ†æé›†(SS)"]
            )
        
        with col3:
            adjustment_vars = st.multiselect(
                "åå˜é‡è°ƒæ•´",
                [col for col in df.columns if col not in [primary_endpoint, 'æ²»ç–—ç»„', 'å—è¯•è€…ID']],
                help="é€‰æ‹©éœ€è¦è°ƒæ•´çš„åå˜é‡"
            )
    
    # æ‰§è¡Œä¸»è¦ç»ˆç‚¹åˆ†æ
    if endpoint_type == "è¿ç»­å‹":
        analyze_continuous_endpoint(df, primary_endpoint, alpha_level, confidence_level, adjustment_vars)
    elif endpoint_type == "äºŒåˆ†ç±»":
        analyze_binary_endpoint(df, primary_endpoint, alpha_level, confidence_level, adjustment_vars)
    elif endpoint_type == "æ—¶é—´-äº‹ä»¶":
        analyze_time_to_event_endpoint(df, primary_endpoint, alpha_level, confidence_level, adjustment_vars)
    elif endpoint_type == "æœ‰åºåˆ†ç±»":
        analyze_ordinal_endpoint(df, primary_endpoint, alpha_level, confidence_level, adjustment_vars)

def identify_endpoint_variables(df, endpoint_type='primary'):
    """è¯†åˆ«ç»ˆç‚¹å˜é‡"""
    if endpoint_type == 'primary':
        keywords = ['ä¸»è¦ç»ˆç‚¹', 'ä¸»ç»ˆç‚¹', 'ç–—æ•ˆ', 'æœ‰æ•ˆç‡', 'ç¼“è§£', 'æ”¹å–„', 'è¾¾æ ‡']
    else:
        keywords = ['æ¬¡è¦ç»ˆç‚¹', 'æ¬¡ç»ˆç‚¹', 'ç”Ÿæ´»è´¨é‡', 'æ»¡æ„åº¦', 'ä¾ä»æ€§', 'å®‰å…¨æ€§']
    
    endpoint_vars = []
    
    for col in df.columns:
        if col in ['å—è¯•è€…ID', 'æ²»ç–—ç»„']:
            continue
        
        # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«å…³é”®è¯
        if any(keyword in col for keyword in keywords):
            endpoint_vars.append(col)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›æ•°å€¼å‹å˜é‡
    if not endpoint_vars:
        endpoint_vars = df.select_dtypes(include=[np.number]).columns.tolist()
        endpoint_vars = [col for col in endpoint_vars if col not in ['å—è¯•è€…ID']]
    
    return endpoint_vars

def analyze_continuous_endpoint(df, endpoint, alpha_level, confidence_level, adjustment_vars):
    """åˆ†æè¿ç»­å‹ä¸»è¦ç»ˆç‚¹"""
    st.markdown("#### ğŸ“Š è¿ç»­å‹ç»ˆç‚¹åˆ†æç»“æœ")
    
    # æè¿°æ€§ç»Ÿè®¡
    st.markdown("##### ğŸ“‹ æè¿°æ€§ç»Ÿè®¡")
    
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    desc_stats = []
    
    for group in treatment_groups:
        group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
        
        if len(group_data) > 0:
            desc_stats.append({
                'æ²»ç–—ç»„': group,
                'ä¾‹æ•°': len(group_data),
                'å‡å€¼': group_data.mean(),
                'æ ‡å‡†å·®': group_data.std(),
                'ä¸­ä½æ•°': group_data.median(),
                'æœ€å°å€¼': group_data.min(),
                'æœ€å¤§å€¼': group_data.max(),
                f'{confidence_level*100:.0f}%ç½®ä¿¡åŒºé—´ä¸‹é™': group_data.mean() - stats.t.ppf(1-alpha_level/2, len(group_data)-1) * group_data.sem(),
                f'{confidence_level*100:.0f}%ç½®ä¿¡åŒºé—´ä¸Šé™': group_data.mean() + stats.t.ppf(1-alpha_level/2, len(group_data)-1) * group_data.sem()
            })
    
    desc_df = pd.DataFrame(desc_stats)
    st.dataframe(desc_df.round(3), use_container_width=True)
    
    # ç»Ÿè®¡æ£€éªŒ
    st.markdown("##### ğŸ§® ç»Ÿè®¡æ£€éªŒ")
    
    if len(treatment_groups) == 2:
        # ä¸¤ç»„æ¯”è¾ƒ
        group1_data = df[df['æ²»ç–—ç»„'] == treatment_groups[0]][endpoint].dropna()
        group2_data = df[df['æ²»ç–—ç»„'] == treatment_groups[1]][endpoint].dropna()
        
        # é€‰æ‹©æ£€éªŒæ–¹æ³•
        if (is_normally_distributed(group1_data) and is_normally_distributed(group2_data) 
            and len(group1_data) >= 30 and len(group2_data) >= 30):
            
            # tæ£€éªŒ
            t_stat, p_value = ttest_ind(group1_data, group2_data)
            test_method = "ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ"
            
                        # è®¡ç®—æ•ˆåº”é‡å’Œç½®ä¿¡åŒºé—´
            mean_diff = group1_data.mean() - group2_data.mean()
            pooled_se = np.sqrt(group1_data.var()/len(group1_data) + group2_data.var()/len(group2_data))
            
            # ç½®ä¿¡åŒºé—´
            df_welch = (group1_data.var()/len(group1_data) + group2_data.var()/len(group2_data))**2 / (
                (group1_data.var()/len(group1_data))**2/(len(group1_data)-1) + 
                (group2_data.var()/len(group2_data))**2/(len(group2_data)-1)
            )
            
            t_critical = stats.t.ppf(1-alpha_level/2, df_welch)
            ci_lower = mean_diff - t_critical * pooled_se
            ci_upper = mean_diff + t_critical * pooled_se
            
            # Cohen's d
            cohens_d = calculate_cohens_d(group1_data, group2_data)
            
        else:
            # Mann-Whitney Uæ£€éªŒ
            u_stat, p_value = mannwhitneyu(group1_data, group2_data, alternative='two-sided')
            test_method = "Mann-Whitney Uæ£€éªŒ"
            
            # ä¸­ä½æ•°å·®å¼‚
            median_diff = group1_data.median() - group2_data.median()
            
            # æ•ˆåº”é‡ (r = Z/sqrt(N))
            z_score = stats.norm.ppf(1 - p_value/2)
            effect_size_r = abs(z_score) / np.sqrt(len(group1_data) + len(group2_data))
            
        # æ˜¾ç¤ºæ£€éªŒç»“æœ
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**æ£€éªŒç»Ÿè®¡é‡:**")
            if test_method == "ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ":
                st.write(f"â€¢ æ£€éªŒæ–¹æ³•: {test_method}")
                st.write(f"â€¢ tç»Ÿè®¡é‡: {t_stat:.4f}")
                st.write(f"â€¢ På€¼: {p_value:.4f}")
                st.write(f"â€¢ å‡å€¼å·®å¼‚: {mean_diff:.3f}")
                st.write(f"â€¢ {confidence_level*100:.0f}%ç½®ä¿¡åŒºé—´: ({ci_lower:.3f}, {ci_upper:.3f})")
                st.write(f"â€¢ Cohen's d: {cohens_d:.3f}")
            else:
                st.write(f"â€¢ æ£€éªŒæ–¹æ³•: {test_method}")
                st.write(f"â€¢ Uç»Ÿè®¡é‡: {u_stat:.4f}")
                st.write(f"â€¢ På€¼: {p_value:.4f}")
                st.write(f"â€¢ ä¸­ä½æ•°å·®å¼‚: {median_diff:.3f}")
                st.write(f"â€¢ æ•ˆåº”é‡(r): {effect_size_r:.3f}")
        
        with col2:
            # ç»“æœè§£é‡Š
            st.markdown("**ç»“æœè§£é‡Š:**")
            if p_value < alpha_level:
                st.success(f"âœ… åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œä¸¤ç»„é—´å·®å¼‚å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰")
            else:
                st.info(f"â„¹ï¸ åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œä¸¤ç»„é—´å·®å¼‚æ— ç»Ÿè®¡å­¦æ„ä¹‰")
            
            # æ•ˆåº”é‡è§£é‡Š
            if test_method == "ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ":
                if abs(cohens_d) < 0.2:
                    effect_interpretation = "æ•ˆåº”é‡å¾ˆå°"
                elif abs(cohens_d) < 0.5:
                    effect_interpretation = "æ•ˆåº”é‡å°"
                elif abs(cohens_d) < 0.8:
                    effect_interpretation = "æ•ˆåº”é‡ä¸­ç­‰"
                else:
                    effect_interpretation = "æ•ˆåº”é‡å¤§"
                st.write(f"â€¢ {effect_interpretation}")
    
    else:
        # å¤šç»„æ¯”è¾ƒ
        group_data_list = []
        for group in treatment_groups:
            group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
            group_data_list.append(group_data)
        
        # æ£€æŸ¥æ­£æ€æ€§
        all_normal = all(is_normally_distributed(data) for data in group_data_list if len(data) >= 8)
        
        if all_normal:
            # æ–¹å·®åˆ†æ
            f_stat, p_value = stats.f_oneway(*group_data_list)
            test_method = "å•å› ç´ æ–¹å·®åˆ†æ(ANOVA)"
            
            # è®¡ç®—æ•ˆåº”é‡ (eta squared)
            ss_between = sum(len(data) * (data.mean() - df[endpoint].mean())**2 for data in group_data_list)
            ss_total = sum((df[endpoint] - df[endpoint].mean())**2)
            eta_squared = ss_between / ss_total
            
        else:
            # Kruskal-Wallisæ£€éªŒ
            h_stat, p_value = stats.kruskal(*group_data_list)
            test_method = "Kruskal-Wallisæ£€éªŒ"
            
            # æ•ˆåº”é‡ (epsilon squared)
            n_total = sum(len(data) for data in group_data_list)
            epsilon_squared = (h_stat - len(treatment_groups) + 1) / (n_total - len(treatment_groups))
        
        # æ˜¾ç¤ºå¤šç»„æ¯”è¾ƒç»“æœ
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**æ£€éªŒç»Ÿè®¡é‡:**")
            if test_method == "å•å› ç´ æ–¹å·®åˆ†æ(ANOVA)":
                st.write(f"â€¢ æ£€éªŒæ–¹æ³•: {test_method}")
                st.write(f"â€¢ Fç»Ÿè®¡é‡: {f_stat:.4f}")
                st.write(f"â€¢ På€¼: {p_value:.4f}")
                st.write(f"â€¢ æ•ˆåº”é‡(Î·Â²): {eta_squared:.3f}")
            else:
                st.write(f"â€¢ æ£€éªŒæ–¹æ³•: {test_method}")
                st.write(f"â€¢ Hç»Ÿè®¡é‡: {h_stat:.4f}")
                st.write(f"â€¢ På€¼: {p_value:.4f}")
                st.write(f"â€¢ æ•ˆåº”é‡(ÎµÂ²): {epsilon_squared:.3f}")
        
        with col2:
            st.markdown("**ç»“æœè§£é‡Š:**")
            if p_value < alpha_level:
                st.success(f"âœ… åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œå„ç»„é—´å·®å¼‚å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰")
                
                # äº‹åå¤šé‡æ¯”è¾ƒ
                if st.checkbox("è¿›è¡Œäº‹åå¤šé‡æ¯”è¾ƒ"):
                    perform_post_hoc_analysis(df, endpoint, treatment_groups, alpha_level)
            else:
                st.info(f"â„¹ï¸ åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œå„ç»„é—´å·®å¼‚æ— ç»Ÿè®¡å­¦æ„ä¹‰")
    
    # åå˜é‡è°ƒæ•´åˆ†æ
    if adjustment_vars:
        st.markdown("##### ğŸ”§ åå˜é‡è°ƒæ•´åˆ†æ")
        perform_covariate_adjustment(df, endpoint, adjustment_vars, alpha_level)
    
    # å¯è§†åŒ–
    st.markdown("##### ğŸ“Š ç»“æœå¯è§†åŒ–")
    create_endpoint_visualization(df, endpoint, treatment_groups)

def analyze_binary_endpoint(df, endpoint, alpha_level, confidence_level, adjustment_vars):
    """åˆ†æäºŒåˆ†ç±»ä¸»è¦ç»ˆç‚¹"""
    st.markdown("#### ğŸ¯ äºŒåˆ†ç±»ç»ˆç‚¹åˆ†æç»“æœ")
    
    # æè¿°æ€§ç»Ÿè®¡
    st.markdown("##### ğŸ“‹ æè¿°æ€§ç»Ÿè®¡")
    
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    binary_stats = []
    
    for group in treatment_groups:
        group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
        
        if len(group_data) > 0:
            success_count = group_data.sum() if group_data.dtype in [bool, 'bool'] else (group_data == 1).sum()
            total_count = len(group_data)
            success_rate = success_count / total_count
            
            # è®¡ç®—95%ç½®ä¿¡åŒºé—´ (Wilsonæ–¹æ³•)
            z = stats.norm.ppf(1 - alpha_level/2)
            n = total_count
            p = success_rate
            
            denominator = 1 + z**2/n
            center = (p + z**2/(2*n)) / denominator
            half_width = z * np.sqrt((p*(1-p) + z**2/(4*n))/n) / denominator
            
            ci_lower = max(0, center - half_width)
            ci_upper = min(1, center + half_width)
            
            binary_stats.append({
                'æ²»ç–—ç»„': group,
                'æ€»ä¾‹æ•°': total_count,
                'æˆåŠŸä¾‹æ•°': success_count,
                'æˆåŠŸç‡(%)': success_rate * 100,
                f'{confidence_level*100:.0f}%ç½®ä¿¡åŒºé—´ä¸‹é™(%)': ci_lower * 100,
                f'{confidence_level*100:.0f}%ç½®ä¿¡åŒºé—´ä¸Šé™(%)': ci_upper * 100
            })
    
    binary_df = pd.DataFrame(binary_stats)
    st.dataframe(binary_df.round(2), use_container_width=True)
    
    # ç»Ÿè®¡æ£€éªŒ
    st.markdown("##### ğŸ§® ç»Ÿè®¡æ£€éªŒ")
    
    if len(treatment_groups) == 2:
        # ä¸¤ç»„æ¯”è¾ƒ
        group1_data = df[df['æ²»ç–—ç»„'] == treatment_groups[0]][endpoint].dropna()
        group2_data = df[df['æ²»ç–—ç»„'] == treatment_groups[1]][endpoint].dropna()
        
        # åˆ›å»º2x2åˆ—è”è¡¨
        success1 = group1_data.sum() if group1_data.dtype in [bool, 'bool'] else (group1_data == 1).sum()
        success2 = group2_data.sum() if group2_data.dtype in [bool, 'bool'] else (group2_data == 1).sum()
        
        total1, total2 = len(group1_data), len(group2_data)
        fail1, fail2 = total1 - success1, total2 - success2
        
        contingency_table = np.array([[success1, fail1], [success2, fail2]])
        
        # é€‰æ‹©æ£€éªŒæ–¹æ³•
        if min(contingency_table.flatten()) >= 5:
            # å¡æ–¹æ£€éªŒ
            chi2, p_value, _, _ = chi2_contingency(contingency_table)
            test_method = "å¡æ–¹æ£€éªŒ"
        else:
            # Fisherç²¾ç¡®æ£€éªŒ
            _, p_value = fisher_exact(contingency_table)
            test_method = "Fisherç²¾ç¡®æ£€éªŒ"
        
        # è®¡ç®—æ•ˆåº”é‡å’Œé£é™©æŒ‡æ ‡
        rate1 = success1 / total1
        rate2 = success2 / total2
        
        # ç›¸å¯¹é£é™© (RR)
        rr = rate1 / rate2 if rate2 > 0 else float('inf')
        
        # é£é™©å·® (RD)
        rd = rate1 - rate2
        
        # æ¯”å€¼æ¯” (OR)
        if fail1 > 0 and fail2 > 0:
            or_value = (success1 * fail2) / (fail1 * success2)
        else:
            or_value = float('inf')
        
        # éœ€è¦æ²»ç–—çš„ç—…äººæ•° (NNT)
        nnt = 1 / abs(rd) if rd != 0 else float('inf')
        
        # æ˜¾ç¤ºæ£€éªŒç»“æœ
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**æ£€éªŒç»Ÿè®¡é‡:**")
            st.write(f"â€¢ æ£€éªŒæ–¹æ³•: {test_method}")
            if test_method == "å¡æ–¹æ£€éªŒ":
                st.write(f"â€¢ Ï‡Â²ç»Ÿè®¡é‡: {chi2:.4f}")
            st.write(f"â€¢ På€¼: {p_value:.4f}")
            
            st.markdown("**æ•ˆåº”é‡æŒ‡æ ‡:**")
            st.write(f"â€¢ ç›¸å¯¹é£é™©(RR): {rr:.3f}")
            st.write(f"â€¢ é£é™©å·®(RD): {rd:.3f}")
            st.write(f"â€¢ æ¯”å€¼æ¯”(OR): {or_value:.3f}")
            if nnt != float('inf'):
                st.write(f"â€¢ éœ€è¦æ²»ç–—çš„ç—…äººæ•°(NNT): {nnt:.1f}")
        
        with col2:
            st.markdown("**ç»“æœè§£é‡Š:**")
            if p_value < alpha_level:
                st.success(f"âœ… åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œä¸¤ç»„æˆåŠŸç‡å·®å¼‚å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰")
            else:
                st.info(f"â„¹ï¸ åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œä¸¤ç»„æˆåŠŸç‡å·®å¼‚æ— ç»Ÿè®¡å­¦æ„ä¹‰")
            
            # ä¸´åºŠæ„ä¹‰è§£é‡Š
            if rr > 1:
                st.write(f"â€¢ è¯•éªŒç»„æˆåŠŸç‡æ˜¯å¯¹ç…§ç»„çš„{rr:.2f}å€")
            elif rr < 1:
                st.write(f"â€¢ è¯•éªŒç»„æˆåŠŸç‡æ˜¯å¯¹ç…§ç»„çš„{rr:.2f}å€ï¼ˆé™ä½ï¼‰")
            
            if rd > 0:
                st.write(f"â€¢ è¯•éªŒç»„æˆåŠŸç‡æ¯”å¯¹ç…§ç»„é«˜{abs(rd)*100:.1f}ä¸ªç™¾åˆ†ç‚¹")
            elif rd < 0:
                st.write(f"â€¢ è¯•éªŒç»„æˆåŠŸç‡æ¯”å¯¹ç…§ç»„ä½{abs(rd)*100:.1f}ä¸ªç™¾åˆ†ç‚¹")
    
    else:
        # å¤šç»„æ¯”è¾ƒ - å¡æ–¹æ£€éªŒ
        contingency_table = pd.crosstab(df[endpoint], df['æ²»ç–—ç»„'])
        chi2, p_value, _, _ = chi2_contingency(contingency_table)
        
        st.markdown("**å¤šç»„æ¯”è¾ƒç»“æœ:**")
        col1, col2 = st.columns(2)
        
        with col1:
            st.write(f"â€¢ æ£€éªŒæ–¹æ³•: å¡æ–¹æ£€éªŒ")
            st.write(f"â€¢ Ï‡Â²ç»Ÿè®¡é‡: {chi2:.4f}")
            st.write(f"â€¢ På€¼: {p_value:.4f}")
            st.write(f"â€¢ è‡ªç”±åº¦: {(contingency_table.shape[0]-1)*(contingency_table.shape[1]-1)}")
        
        with col2:
            if p_value < alpha_level:
                st.success(f"âœ… åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œå„ç»„æˆåŠŸç‡å·®å¼‚å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰")
            else:
                st.info(f"â„¹ï¸ åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œå„ç»„æˆåŠŸç‡å·®å¼‚æ— ç»Ÿè®¡å­¦æ„ä¹‰")
    
    # å¯è§†åŒ–
    st.markdown("##### ğŸ“Š ç»“æœå¯è§†åŒ–")
    create_binary_endpoint_visualization(df, endpoint, treatment_groups)

def analyze_time_to_event_endpoint(df, endpoint, alpha_level, confidence_level, adjustment_vars):
    """åˆ†ææ—¶é—´-äº‹ä»¶ç»ˆç‚¹"""
    st.markdown("#### â±ï¸ æ—¶é—´-äº‹ä»¶ç»ˆç‚¹åˆ†æç»“æœ")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç”Ÿå­˜æ—¶é—´å’Œäº‹ä»¶çŠ¶æ€åˆ—
    time_col = st.selectbox("é€‰æ‹©æ—¶é—´å˜é‡", df.select_dtypes(include=[np.number]).columns.tolist())
    event_col = st.selectbox("é€‰æ‹©äº‹ä»¶çŠ¶æ€å˜é‡", df.columns.tolist())
    
    if not time_col or not event_col:
        st.warning("âš ï¸ æ—¶é—´-äº‹ä»¶åˆ†æéœ€è¦æ—¶é—´å˜é‡å’Œäº‹ä»¶çŠ¶æ€å˜é‡")
        return
    
    try:
        from lifelines import KaplanMeierFitter, logrank_test
        from lifelines.statistics import multivariate_logrank_test
        
        # ç”Ÿå­˜åˆ†æ
        treatment_groups = df['æ²»ç–—ç»„'].unique()
        
        # Kaplan-Meierç”Ÿå­˜æ›²çº¿
        st.markdown("##### ğŸ“ˆ Kaplan-Meierç”Ÿå­˜æ›²çº¿")
        
        fig = go.Figure()
        survival_stats = []
        
        for group in treatment_groups:
            group_data = df[df['æ²»ç–—ç»„'] == group]
            
            if len(group_data) > 0:
                kmf = KaplanMeierFitter()
                kmf.fit(group_data[time_col], group_data[event_col], label=group)
                
                # æ·»åŠ ç”Ÿå­˜æ›²çº¿
                fig.add_trace(go.Scatter(
                    x=kmf.timeline,
                    y=kmf.survival_function_[group],
                    mode='lines',
                    name=group,
                    line=dict(width=2)
                ))
                
                # è®¡ç®—ç”Ÿå­˜ç»Ÿè®¡
                median_survival = kmf.median_survival_time_
                survival_at_times = []
                
                for t in [12, 24, 36]:  # 1å¹´ã€2å¹´ã€3å¹´ç”Ÿå­˜ç‡
                    if t <= kmf.timeline.max():
                        survival_rate = kmf.survival_function_at_times(t).iloc[0]
                        survival_at_times.append(f"{t}ä¸ªæœˆ: {survival_rate:.3f}")
                
                survival_stats.append({
                    'æ²»ç–—ç»„': group,
                    'ä¾‹æ•°': len(group_data),
                    'äº‹ä»¶æ•°': group_data[event_col].sum(),
                    'ä¸­ä½ç”Ÿå­˜æ—¶é—´': median_survival if not np.isnan(median_survival) else "æœªè¾¾åˆ°",
                    'ç”Ÿå­˜ç‡': "; ".join(survival_at_times)
                })
        
        fig.update_layout(
            title="Kaplan-Meierç”Ÿå­˜æ›²çº¿",
            xaxis_title="æ—¶é—´",
            yaxis_title="ç”Ÿå­˜æ¦‚ç‡",
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ç”Ÿå­˜ç»Ÿè®¡è¡¨
        st.markdown("##### ğŸ“‹ ç”Ÿå­˜ç»Ÿè®¡")
        survival_df = pd.DataFrame(survival_stats)
        st.dataframe(survival_df, use_container_width=True)
        
        # Log-rankæ£€éªŒ
        st.markdown("##### ğŸ§® Log-rankæ£€éªŒ")
        
        if len(treatment_groups) == 2:
            # ä¸¤ç»„æ¯”è¾ƒ
            group1_data = df[df['æ²»ç–—ç»„'] == treatment_groups[0]]
            group2_data = df[df['æ²»ç–—ç»„'] == treatment_groups[1]]
            
            results = logrank_test(
                group1_data[time_col], group2_data[time_col],
                group1_data[event_col], group2_data[event_col]
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Log-rankæ£€éªŒç»“æœ:**")
                st.write(f"â€¢ æ£€éªŒç»Ÿè®¡é‡: {results.test_statistic:.4f}")
                st.write(f"â€¢ På€¼: {results.p_value:.4f}")
                st.write(f"â€¢ è‡ªç”±åº¦: 1")
            
            with col2:
                if results.p_value < alpha_level:
                    st.success(f"âœ… åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œä¸¤ç»„ç”Ÿå­˜å·®å¼‚å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰")
                else:
                    st.info(f"â„¹ï¸ åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œä¸¤ç»„ç”Ÿå­˜å·®å¼‚æ— ç»Ÿè®¡å­¦æ„ä¹‰")
        
        else:
            # å¤šç»„æ¯”è¾ƒ
            results = multivariate_logrank_test(df[time_col], df['æ²»ç–—ç»„'], df[event_col])
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**å¤šç»„Log-rankæ£€éªŒç»“æœ:**")
                st.write(f"â€¢ æ£€éªŒç»Ÿè®¡é‡: {results.test_statistic:.4f}")
                st.write(f"â€¢ På€¼: {results.p_value:.4f}")
                st.write(f"â€¢ è‡ªç”±åº¦: {len(treatment_groups)-1}")
            
            with col2:
                if results.p_value < alpha_level:
                    st.success(f"âœ… åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œå„ç»„ç”Ÿå­˜å·®å¼‚å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰")
                else:
                    st.info(f"â„¹ï¸ åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œå„ç»„ç”Ÿå­˜å·®å¼‚æ— ç»Ÿè®¡å­¦æ„ä¹‰")
        
        # Coxå›å½’åˆ†æ
        if adjustment_vars:
            st.markdown("##### ğŸ”§ Coxæ¯”ä¾‹é£é™©å›å½’")
            perform_cox_regression(df, time_col, event_col, adjustment_vars)
            
    except ImportError:
        st.error("âŒ éœ€è¦å®‰è£…lifelinesåº“è¿›è¡Œç”Ÿå­˜åˆ†æ")
        st.code("pip install lifelines")

def perform_post_hoc_analysis(df, endpoint, treatment_groups, alpha_level):
    """æ‰§è¡Œäº‹åå¤šé‡æ¯”è¾ƒ"""
    st.markdown("**äº‹åå¤šé‡æ¯”è¾ƒ (Tukey HSD):**")
    
    try:
        from scipy.stats import tukey_hsd
        
        # å‡†å¤‡æ•°æ®
        group_data_list = []
        for group in treatment_groups:
            group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
            group_data_list.append(group_data)
        
        # Tukey HSDæ£€éªŒ
        tukey_result = tukey_hsd(*group_data_list)
        
        # åˆ›å»ºæ¯”è¾ƒç»“æœè¡¨
        comparisons = []
        for i in range(len(treatment_groups)):
            for j in range(i+1, len(treatment_groups)):
                p_value = tukey_result.pvalue[i, j]
                mean_diff = group_data_list[i].mean() - group_data_list[j].mean()
                
                comparisons.append({
                    'æ¯”è¾ƒç»„': f"{treatment_groups[i]} vs {treatment_groups[j]}",
                    'å‡å€¼å·®å¼‚': mean_diff,
                    'På€¼': p_value,
                    'æ˜¾è‘—æ€§': "æ˜¯" if p_value < alpha_level else "å¦"
                })
        
        comparison_df = pd.DataFrame(comparisons)
        st.dataframe(comparison_df.round(4), use_container_width=True)
        
    except ImportError:
        st.warning("âš ï¸ æ— æ³•è¿›è¡ŒTukey HSDæ£€éªŒï¼Œè¯·å‡çº§scipyç‰ˆæœ¬")

def perform_covariate_adjustment(df, endpoint, adjustment_vars, alpha_level):
    """æ‰§è¡Œåå˜é‡è°ƒæ•´åˆ†æ"""
    try:
        import statsmodels.api as sm
        from statsmodels.formula.api import ols
        
        # æ„å»ºå›å½’å…¬å¼
        formula = f"{endpoint} ~ C(æ²»ç–—ç»„)"
        
        for var in adjustment_vars:
            if df[var].dtype in ['object', 'category']:
                formula += f" + C({var})"
            else:
                formula += f" + {var}"
        
        # æ‹Ÿåˆæ¨¡å‹
        model = ols(formula, data=df).fit()
        
        # æ˜¾ç¤ºç»“æœ
        st.markdown("**åå˜é‡è°ƒæ•´åçš„ç»“æœ:**")
        
        # æå–æ²»ç–—ç»„æ•ˆåº”
        treatment_params = [param for param in model.params.index if 'æ²»ç–—ç»„' in param]
        
        if treatment_params:
            for param in treatment_params:
                coef = model.params[param]
                se = model.bse[param]
                p_value = model.pvalues[param]
                ci_lower = model.conf_int().loc[param, 0]
                ci_upper = model.conf_int().loc[param, 1]
                
                st.write(f"â€¢ {param}: ç³»æ•°={coef:.3f}, SE={se:.3f}, P={p_value:.4f}")
                st.write(f"  95%ç½®ä¿¡åŒºé—´: ({ci_lower:.3f}, {ci_upper:.3f})")
        
        # æ¨¡å‹æ‹Ÿåˆä¼˜åº¦
        st.write(f"â€¢ RÂ²: {model.rsquared:.3f}")
        st.write(f"â€¢ è°ƒæ•´RÂ²: {model.rsquared_adj:.3f}")
        st.write(f"â€¢ Fç»Ÿè®¡é‡På€¼: {model.f_pvalue:.4f}")
        
    except ImportError:
        st.warning("âš ï¸ éœ€è¦å®‰è£…statsmodelsåº“è¿›è¡Œåå˜é‡è°ƒæ•´")

def create_endpoint_visualization(df, endpoint, treatment_groups):
    """åˆ›å»ºç»ˆç‚¹å¯è§†åŒ–"""
    col1, col2 = st.columns(2)
    
    with col1:
        # ç®±çº¿å›¾
        fig_box = px.box(
            df, x='æ²»ç–—ç»„', y=endpoint,
            title=f"{endpoint} ç»„é—´æ¯”è¾ƒ",
            points="outliers"
        )
        fig_box.update_layout(height=400)
        st.plotly_chart(fig_box, use_container_width=True)
    
    with col2:
        # å°æç´å›¾
        fig_violin = px.violin(
            df, x='æ²»ç–—ç»„', y=endpoint,
            title=f"{endpoint} åˆ†å¸ƒæ¯”è¾ƒ",
            box=True
        )
        fig_violin.update_layout(height=400)
        st.plotly_chart(fig_violin, use_container_width=True)

def create_binary_endpoint_visualization(df, endpoint, treatment_groups):
    """åˆ›å»ºäºŒåˆ†ç±»ç»ˆç‚¹å¯è§†åŒ–"""
    # è®¡ç®—æˆåŠŸç‡
    success_rates = []
    for group in treatment_groups:
        group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
        if len(group_data) > 0:
            success_count = group_data.sum() if group_data.dtype in [bool, 'bool'] else (group_data == 1).sum()
            success_rate = success_count / len(group_data) * 100
            success_rates.append({'æ²»ç–—ç»„': group, 'æˆåŠŸç‡(%)': success_rate})
    
    success_df = pd.DataFrame(success_rates)
    
    col1, col2 = st.columns(2)
    
    with col1:
        # æŸ±çŠ¶å›¾
        fig_bar = px.bar(
            success_df, x='æ²»ç–—ç»„', y='æˆåŠŸç‡(%)',
            title="å„ç»„æˆåŠŸç‡æ¯”è¾ƒ",
            color='æ²»ç–—ç»„'
        )
        fig_bar.update_layout(height=400)
        st.plotly_chart(fig_bar, use_container_width=True)
    
    with col2:
        # é¥¼å›¾ï¼ˆå¦‚æœåªæœ‰ä¸¤ç»„ï¼‰
        if len(treatment_groups) == 2:
            fig_pie = px.pie(
                success_df, values='æˆåŠŸç‡(%)', names='æ²»ç–—ç»„',
                title="æˆåŠŸç‡åˆ†å¸ƒ"
            )
            fig_pie.update_layout(height=400)
            st.plotly_chart(fig_pie, use_container_width=True)
        else:
            # å †ç§¯æŸ±çŠ¶å›¾æ˜¾ç¤ºæˆåŠŸ/å¤±è´¥
            stacked_data = []
            for group in treatment_groups:
                group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
                if len(group_data) > 0:
                    success_count = group_data.sum() if group_data.dtype in [bool, 'bool'] else (group_data == 1).sum()
                    fail_count = len(group_data) - success_count
                    
                    stacked_data.extend([
                        {'æ²»ç–—ç»„': group, 'ç»“æœ': 'æˆåŠŸ', 'äººæ•°': success_count},
                        {'æ²»ç–—ç»„': group, 'ç»“æœ': 'å¤±è´¥', 'äººæ•°': fail_count}
                    ])
            
            stacked_df = pd.DataFrame(stacked_data)
            fig_stacked = px.bar(
                stacked_df, x='æ²»ç–—ç»„', y='äººæ•°', color='ç»“æœ',
                title="æˆåŠŸ/å¤±è´¥äººæ•°åˆ†å¸ƒ",
                barmode='stack'
            )
            fig_stacked.update_layout(height=400)
            st.plotly_chart(fig_stacked, use_container_width=True)

def secondary_endpoint_analysis(df):
    """æ¬¡è¦ç»ˆç‚¹åˆ†æ"""
    st.markdown("### ğŸ“ˆ æ¬¡è¦ç»ˆç‚¹åˆ†æ")
    st.markdown("*åˆ†æè¯•éªŒçš„æ¬¡è¦ç–—æ•ˆç»ˆç‚¹å’Œæ¢ç´¢æ€§ç»ˆç‚¹*")
    
    # è¯†åˆ«æ¬¡è¦ç»ˆç‚¹å˜é‡
    secondary_vars = identify_endpoint_variables(df, endpoint_type='secondary')
    
    if not secondary_vars:
        st.warning("âš ï¸ æœªè¯†åˆ«åˆ°æ¬¡è¦ç»ˆç‚¹å˜é‡")
        return
    
    # é€‰æ‹©æ¬¡è¦ç»ˆç‚¹
    selected_endpoints = st.multiselect(
        "é€‰æ‹©æ¬¡è¦ç»ˆç‚¹å˜é‡",
        secondary_vars,
        default=secondary_vars[:5] if len(secondary_vars) >= 5 else secondary_vars,
        help="å¯ä»¥é€‰æ‹©å¤šä¸ªæ¬¡è¦ç»ˆç‚¹è¿›è¡Œåˆ†æ"
    )
    
    if not selected_endpoints:
        return
    
    # åˆ†æè®¾ç½®
    with st.expander("ğŸ”§ åˆ†æè®¾ç½®", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            alpha_level = st.selectbox("æ˜¾è‘—æ€§æ°´å¹³", [0.05, 0.01, 0.001], index=0)
            multiple_comparison = st.checkbox("å¤šé‡æ¯”è¾ƒæ ¡æ­£", value=True)
        
        with col2:
            correction_method = st.selectbox(
                "æ ¡æ­£æ–¹æ³•",
                ["Bonferroni", "Holm", "FDR (Benjamini-Hochberg)"],
                disabled=not multiple_comparison
            )
        
        with col3:
                        show_effect_size = st.checkbox("æ˜¾ç¤ºæ•ˆåº”é‡", value=True)
    
    # æ‰§è¡Œæ¬¡è¦ç»ˆç‚¹åˆ†æ
    secondary_results = []
    
    for endpoint in selected_endpoints:
        result = analyze_single_secondary_endpoint(df, endpoint, alpha_level, show_effect_size)
        secondary_results.append(result)
    
    # å¤šé‡æ¯”è¾ƒæ ¡æ­£
    if multiple_comparison and len(secondary_results) > 1:
        secondary_results = apply_multiple_comparison_correction(secondary_results, correction_method, alpha_level)
    
    # æ˜¾ç¤ºç»“æœ
    st.markdown("#### ğŸ“Š æ¬¡è¦ç»ˆç‚¹åˆ†æç»“æœ")
    
    # åˆ›å»ºç»“æœæ±‡æ€»è¡¨
    results_df = pd.DataFrame(secondary_results)
    
    # æ ¼å¼åŒ–ç»“æœè¡¨
    display_columns = ['ç»ˆç‚¹å˜é‡', 'åˆ†æç±»å‹', 'æ£€éªŒæ–¹æ³•']
    
    # æ·»åŠ å„æ²»ç–—ç»„çš„ç»Ÿè®¡é‡
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    for group in treatment_groups:
        if f'{group}_ç»Ÿè®¡é‡' in results_df.columns:
            display_columns.append(f'{group}_ç»Ÿè®¡é‡')
    
    display_columns.extend(['På€¼', 'æ˜¾è‘—æ€§'])
    
    if show_effect_size:
        effect_size_cols = [col for col in results_df.columns if 'æ•ˆåº”é‡' in col]
        display_columns.extend(effect_size_cols)
    
    if multiple_comparison:
        display_columns.extend(['æ ¡æ­£åPå€¼', 'æ ¡æ­£åæ˜¾è‘—æ€§'])
    
    # è¿‡æ»¤å­˜åœ¨çš„åˆ—
    available_columns = [col for col in display_columns if col in results_df.columns]
    display_df = results_df[available_columns]
    
    st.dataframe(display_df, use_container_width=True)
    
    # ç»“æœè§£é‡Š
    st.markdown("#### ğŸ“‹ ç»“æœè§£é‡Š")
    
    significant_endpoints = []
    if multiple_comparison:
        significant_endpoints = [result['ç»ˆç‚¹å˜é‡'] for result in secondary_results 
                               if result.get('æ ¡æ­£åæ˜¾è‘—æ€§') == 'æ˜¯']
    else:
        significant_endpoints = [result['ç»ˆç‚¹å˜é‡'] for result in secondary_results 
                               if result.get('æ˜¾è‘—æ€§') == 'æ˜¯']
    
    if significant_endpoints:
        st.success(f"âœ… å‘ç° {len(significant_endpoints)} ä¸ªå…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰çš„æ¬¡è¦ç»ˆç‚¹:")
        for endpoint in significant_endpoints:
            st.write(f"â€¢ {endpoint}")
    else:
        st.info("â„¹ï¸ æ‰€æœ‰æ¬¡è¦ç»ˆç‚¹å‡æ— ç»Ÿè®¡å­¦æ„ä¹‰")
    
    if multiple_comparison:
        st.info(f"ğŸ’¡ å·²ä½¿ç”¨{correction_method}æ–¹æ³•è¿›è¡Œå¤šé‡æ¯”è¾ƒæ ¡æ­£")
    
    # æ¬¡è¦ç»ˆç‚¹å¯è§†åŒ–
    st.markdown("#### ğŸ“Š æ¬¡è¦ç»ˆç‚¹å¯è§†åŒ–")
    
    # é€‰æ‹©å¯è§†åŒ–çš„ç»ˆç‚¹
    viz_endpoint = st.selectbox("é€‰æ‹©è¦å¯è§†åŒ–çš„ç»ˆç‚¹", selected_endpoints)
    
    if viz_endpoint:
        create_secondary_endpoint_visualization(df, viz_endpoint)
    
    # ç›¸å…³æ€§åˆ†æ
    if len(selected_endpoints) > 1:
        st.markdown("#### ğŸ”— æ¬¡è¦ç»ˆç‚¹ç›¸å…³æ€§åˆ†æ")
        create_endpoint_correlation_analysis(df, selected_endpoints)

def analyze_single_secondary_endpoint(df, endpoint, alpha_level, show_effect_size):
    """åˆ†æå•ä¸ªæ¬¡è¦ç»ˆç‚¹"""
    result = {'ç»ˆç‚¹å˜é‡': endpoint}
    
    # åˆ¤æ–­å˜é‡ç±»å‹
    if df[endpoint].dtype in ['object', 'category'] or df[endpoint].nunique() <= 10:
        result['åˆ†æç±»å‹'] = 'åˆ†ç±»å˜é‡'
        result.update(analyze_categorical_secondary(df, endpoint, alpha_level, show_effect_size))
    else:
        result['åˆ†æç±»å‹'] = 'è¿ç»­å˜é‡'
        result.update(analyze_continuous_secondary(df, endpoint, alpha_level, show_effect_size))
    
    return result

def analyze_categorical_secondary(df, endpoint, alpha_level, show_effect_size):
    """åˆ†æåˆ†ç±»æ¬¡è¦ç»ˆç‚¹"""
    result = {}
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    
    # è®¡ç®—å„ç»„ç»Ÿè®¡é‡
    for group in treatment_groups:
        group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
        if len(group_data) > 0:
            value_counts = group_data.value_counts()
            total = len(group_data)
            
            # æ ¼å¼åŒ–ä¸ºé¢‘æ•°(ç™¾åˆ†æ¯”)
            formatted_values = []
            for value, count in value_counts.items():
                pct = count / total * 100
                formatted_values.append(f"{value}:{count}({pct:.1f}%)")
            
            result[f'{group}_ç»Ÿè®¡é‡'] = "; ".join(formatted_values)
    
    # ç»Ÿè®¡æ£€éªŒ
    try:
        crosstab = pd.crosstab(df[endpoint], df['æ²»ç–—ç»„'])
        
        if crosstab.shape[0] == 2 and crosstab.shape[1] == 2:
            # Fisherç²¾ç¡®æ£€éªŒ
            _, p_value = fisher_exact(crosstab)
            result['æ£€éªŒæ–¹æ³•'] = "Fisherç²¾ç¡®æ£€éªŒ"
        else:
            # å¡æ–¹æ£€éªŒ
            chi2, p_value, _, _ = chi2_contingency(crosstab)
            result['æ£€éªŒæ–¹æ³•'] = "å¡æ–¹æ£€éªŒ"
            
            if show_effect_size:
                # CramÃ©r's V
                n = crosstab.sum().sum()
                cramers_v = np.sqrt(chi2 / (n * (min(crosstab.shape) - 1)))
                result['æ•ˆåº”é‡(CramÃ©r\'s V)'] = f"{cramers_v:.3f}"
        
        result['På€¼'] = f"{p_value:.4f}"
        result['æ˜¾è‘—æ€§'] = "æ˜¯" if p_value < alpha_level else "å¦"
        
    except Exception as e:
        result['æ£€éªŒæ–¹æ³•'] = "è®¡ç®—å¤±è´¥"
        result['På€¼'] = "N/A"
        result['æ˜¾è‘—æ€§'] = "N/A"
    
    return result

def analyze_continuous_secondary(df, endpoint, alpha_level, show_effect_size):
    """åˆ†æè¿ç»­æ¬¡è¦ç»ˆç‚¹"""
    result = {}
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    
    # è®¡ç®—å„ç»„ç»Ÿè®¡é‡
    for group in treatment_groups:
        group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
        if len(group_data) > 0:
            mean = group_data.mean()
            std = group_data.std()
            median = group_data.median()
            
            if is_normally_distributed(group_data):
                result[f'{group}_ç»Ÿè®¡é‡'] = f"{mean:.2f}Â±{std:.2f}"
            else:
                q1 = group_data.quantile(0.25)
                q3 = group_data.quantile(0.75)
                result[f'{group}_ç»Ÿè®¡é‡'] = f"{median:.2f}({q1:.2f},{q3:.2f})"
    
    # ç»Ÿè®¡æ£€éªŒ
    try:
        group_data_list = []
        for group in treatment_groups:
            group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
            group_data_list.append(group_data)
        
        if len(treatment_groups) == 2:
            # ä¸¤ç»„æ¯”è¾ƒ
            group1_data, group2_data = group_data_list[0], group_data_list[1]
            
            if (is_normally_distributed(group1_data) and is_normally_distributed(group2_data) 
                and len(group1_data) >= 30 and len(group2_data) >= 30):
                # tæ£€éªŒ
                _, p_value = ttest_ind(group1_data, group2_data)
                result['æ£€éªŒæ–¹æ³•'] = "tæ£€éªŒ"
                
                if show_effect_size:
                    cohens_d = calculate_cohens_d(group1_data, group2_data)
                    result['æ•ˆåº”é‡(Cohen\'s d)'] = f"{cohens_d:.3f}"
            else:
                # Mann-Whitney Uæ£€éªŒ
                _, p_value = mannwhitneyu(group1_data, group2_data, alternative='two-sided')
                result['æ£€éªŒæ–¹æ³•'] = "Mann-Whitney Uæ£€éªŒ"
                
                if show_effect_size:
                    # æ•ˆåº”é‡r
                    z_score = stats.norm.ppf(1 - p_value/2)
                    effect_size_r = abs(z_score) / np.sqrt(len(group1_data) + len(group2_data))
                    result['æ•ˆåº”é‡(r)'] = f"{effect_size_r:.3f}"
        
        else:
            # å¤šç»„æ¯”è¾ƒ
            all_normal = all(is_normally_distributed(data) for data in group_data_list if len(data) >= 8)
            
            if all_normal:
                # æ–¹å·®åˆ†æ
                _, p_value = stats.f_oneway(*group_data_list)
                result['æ£€éªŒæ–¹æ³•'] = "ANOVA"
                
                if show_effect_size:
                    # eta squared
                    ss_between = sum(len(data) * (data.mean() - df[endpoint].mean())**2 for data in group_data_list)
                    ss_total = sum((df[endpoint] - df[endpoint].mean())**2)
                    eta_squared = ss_between / ss_total
                    result['æ•ˆåº”é‡(Î·Â²)'] = f"{eta_squared:.3f}"
            else:
                # Kruskal-Wallisæ£€éªŒ
                _, p_value = stats.kruskal(*group_data_list)
                result['æ£€éªŒæ–¹æ³•'] = "Kruskal-Wallisæ£€éªŒ"
        
        result['På€¼'] = f"{p_value:.4f}"
        result['æ˜¾è‘—æ€§'] = "æ˜¯" if p_value < alpha_level else "å¦"
        
    except Exception as e:
        result['æ£€éªŒæ–¹æ³•'] = "è®¡ç®—å¤±è´¥"
        result['På€¼'] = "N/A"
        result['æ˜¾è‘—æ€§'] = "N/A"
    
    return result

def apply_multiple_comparison_correction(results, method, alpha_level):
    """åº”ç”¨å¤šé‡æ¯”è¾ƒæ ¡æ­£"""
    from statsmodels.stats.multitest import multipletests
    
    # æå–På€¼
    p_values = []
    for result in results:
        try:
            p_val = float(result['På€¼'])
            p_values.append(p_val)
        except:
            p_values.append(1.0)  # æ— æ³•è®¡ç®—çš„På€¼è®¾ä¸º1
    
    # åº”ç”¨æ ¡æ­£
    if method == "Bonferroni":
        corrected_p = multipletests(p_values, method='bonferroni')[1]
    elif method == "Holm":
        corrected_p = multipletests(p_values, method='holm')[1]
    elif method == "FDR (Benjamini-Hochberg)":
        corrected_p = multipletests(p_values, method='fdr_bh')[1]
    
    # æ›´æ–°ç»“æœ
    for i, result in enumerate(results):
        result['æ ¡æ­£åPå€¼'] = f"{corrected_p[i]:.4f}"
        result['æ ¡æ­£åæ˜¾è‘—æ€§'] = "æ˜¯" if corrected_p[i] < alpha_level else "å¦"
    
    return results

def create_secondary_endpoint_visualization(df, endpoint):
    """åˆ›å»ºæ¬¡è¦ç»ˆç‚¹å¯è§†åŒ–"""
    if df[endpoint].dtype in ['object', 'category'] or df[endpoint].nunique() <= 10:
        # åˆ†ç±»å˜é‡å¯è§†åŒ–
        crosstab = pd.crosstab(df[endpoint], df['æ²»ç–—ç»„'], normalize='columns') * 100
        
        fig = px.bar(
            crosstab.reset_index().melt(id_vars=endpoint, var_name='æ²»ç–—ç»„', value_name='ç™¾åˆ†æ¯”'),
            x=endpoint, y='ç™¾åˆ†æ¯”', color='æ²»ç–—ç»„',
            title=f"{endpoint} åœ¨å„æ²»ç–—ç»„ä¸­çš„åˆ†å¸ƒ",
            barmode='group'
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
        
    else:
        # è¿ç»­å˜é‡å¯è§†åŒ–
        col1, col2 = st.columns(2)
        
        with col1:
            # ç®±çº¿å›¾
            fig_box = px.box(
                df, x='æ²»ç–—ç»„', y=endpoint,
                title=f"{endpoint} ç»„é—´æ¯”è¾ƒ",
                points="outliers"
            )
            fig_box.update_layout(height=400)
            st.plotly_chart(fig_box, use_container_width=True)
        
        with col2:
            # åˆ†å¸ƒå›¾
            fig_hist = px.histogram(
                df, x=endpoint, color='æ²»ç–—ç»„',
                title=f"{endpoint} åˆ†å¸ƒæ¯”è¾ƒ",
                barmode='overlay',
                opacity=0.7
            )
            fig_hist.update_layout(height=400)
            st.plotly_chart(fig_hist, use_container_width=True)

def create_endpoint_correlation_analysis(df, endpoints):
    """åˆ›å»ºç»ˆç‚¹ç›¸å…³æ€§åˆ†æ"""
    # é€‰æ‹©æ•°å€¼å‹ç»ˆç‚¹
    numeric_endpoints = []
    for endpoint in endpoints:
        if df[endpoint].dtype in [np.number] and df[endpoint].nunique() > 10:
            numeric_endpoints.append(endpoint)
    
    if len(numeric_endpoints) < 2:
        st.info("â„¹ï¸ éœ€è¦è‡³å°‘2ä¸ªæ•°å€¼å‹ç»ˆç‚¹è¿›è¡Œç›¸å…³æ€§åˆ†æ")
        return
    
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = df[numeric_endpoints].corr()
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ç›¸å…³ç³»æ•°çƒ­å›¾
        fig_heatmap = px.imshow(
            corr_matrix,
            text_auto=True,
            aspect="auto",
            title="æ¬¡è¦ç»ˆç‚¹ç›¸å…³æ€§çƒ­å›¾",
            color_continuous_scale='RdBu_r'
        )
        fig_heatmap.update_layout(height=400)
        st.plotly_chart(fig_heatmap, use_container_width=True)
    
    with col2:
        # ç›¸å…³ç³»æ•°è¡¨
        st.markdown("**ç›¸å…³ç³»æ•°çŸ©é˜µ:**")
        st.dataframe(corr_matrix.round(3), use_container_width=True)
        
        # æ˜¾è‘—ç›¸å…³çš„ç»ˆç‚¹å¯¹
        significant_pairs = []
        for i in range(len(numeric_endpoints)):
            for j in range(i+1, len(numeric_endpoints)):
                corr_coef = corr_matrix.iloc[i, j]
                if abs(corr_coef) > 0.5:  # ä¸­ç­‰ä»¥ä¸Šç›¸å…³
                    significant_pairs.append({
                        'ç»ˆç‚¹å¯¹': f"{numeric_endpoints[i]} - {numeric_endpoints[j]}",
                        'ç›¸å…³ç³»æ•°': corr_coef
                    })
        
        if significant_pairs:
            st.markdown("**æ˜¾è‘—ç›¸å…³çš„ç»ˆç‚¹å¯¹ (|r| > 0.5):**")
            for pair in significant_pairs:
                st.write(f"â€¢ {pair['ç»ˆç‚¹å¯¹']}: r = {pair['ç›¸å…³ç³»æ•°']:.3f}")

def safety_analysis(df):
    """å®‰å…¨æ€§åˆ†æ"""
    st.markdown("### ğŸ›¡ï¸ å®‰å…¨æ€§åˆ†æ")
    st.markdown("*åˆ†æè¯•éªŒä¸­çš„ä¸è‰¯äº‹ä»¶å’Œå®‰å…¨æ€§æŒ‡æ ‡*")
    
    # è¯†åˆ«å®‰å…¨æ€§å˜é‡
    safety_vars = identify_safety_variables(df)
    
    if not safety_vars:
        st.warning("âš ï¸ æœªè¯†åˆ«åˆ°å®‰å…¨æ€§ç›¸å…³å˜é‡")
        st.info("ğŸ’¡ è¯·ç¡®ä¿æ•°æ®ä¸­åŒ…å«ä¸è‰¯äº‹ä»¶ã€å®éªŒå®¤æ£€æŸ¥ç­‰å®‰å…¨æ€§æŒ‡æ ‡")
        return
    
    # å®‰å…¨æ€§åˆ†æç±»å‹é€‰æ‹©
    safety_analysis_type = st.selectbox(
        "é€‰æ‹©å®‰å…¨æ€§åˆ†æç±»å‹",
        ["ä¸è‰¯äº‹ä»¶åˆ†æ", "å®éªŒå®¤æ£€æŸ¥åˆ†æ", "ç”Ÿå‘½ä½“å¾åˆ†æ", "ä¸¥é‡ä¸è‰¯äº‹ä»¶åˆ†æ", "å®‰å…¨æ€§æ€»ç»“"]
    )
    
    if safety_analysis_type == "ä¸è‰¯äº‹ä»¶åˆ†æ":
        adverse_events_analysis(df, safety_vars)
    elif safety_analysis_type == "å®éªŒå®¤æ£€æŸ¥åˆ†æ":
        laboratory_analysis(df, safety_vars)
    elif safety_analysis_type == "ç”Ÿå‘½ä½“å¾åˆ†æ":
        vital_signs_analysis(df, safety_vars)
    elif safety_analysis_type == "ä¸¥é‡ä¸è‰¯äº‹ä»¶åˆ†æ":
        serious_adverse_events_analysis(df, safety_vars)
    elif safety_analysis_type == "å®‰å…¨æ€§æ€»ç»“":
        safety_summary_analysis(df, safety_vars)

def identify_safety_variables(df):
    """è¯†åˆ«å®‰å…¨æ€§å˜é‡"""
    safety_keywords = [
        'ä¸è‰¯äº‹ä»¶', 'AE', 'SAE', 'ä¸¥é‡ä¸è‰¯äº‹ä»¶', 'å‰¯ä½œç”¨', 'ä¸è‰¯ååº”',
        'å®éªŒå®¤', 'è¡€å¸¸è§„', 'ç”ŸåŒ–', 'è‚åŠŸèƒ½', 'è‚¾åŠŸèƒ½', 
        'è¡€å‹', 'å¿ƒç‡', 'ä½“æ¸©', 'å‘¼å¸', 'ç”Ÿå‘½ä½“å¾',
        'å®‰å…¨æ€§', 'è€å—æ€§', 'æ¯’æ€§'
    ]
    
    safety_vars = []
    
    for col in df.columns:
        if col in ['å—è¯•è€…ID', 'æ²»ç–—ç»„']:
            continue
        
        # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«å®‰å…¨æ€§å…³é”®è¯
        if any(keyword in col for keyword in safety_keywords):
            safety_vars.append(col)
    
    return safety_vars

def adverse_events_analysis(df, safety_vars):
    """ä¸è‰¯äº‹ä»¶åˆ†æ"""
    st.markdown("#### ğŸš¨ ä¸è‰¯äº‹ä»¶åˆ†æ")
    
    # é€‰æ‹©ä¸è‰¯äº‹ä»¶å˜é‡
    ae_vars = st.multiselect(
        "é€‰æ‹©ä¸è‰¯äº‹ä»¶å˜é‡",
        safety_vars,
        help="é€‰æ‹©åŒ…å«ä¸è‰¯äº‹ä»¶ä¿¡æ¯çš„å˜é‡"
    )
    
    if not ae_vars:
        return
    
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    
    # ä¸è‰¯äº‹ä»¶å‘ç”Ÿç‡åˆ†æ
    st.markdown("##### ğŸ“Š ä¸è‰¯äº‹ä»¶å‘ç”Ÿç‡")
    
    ae_summary = []
    
    for ae_var in ae_vars:
        for group in treatment_groups:
            group_data = df[df['æ²»ç–—ç»„'] == group]
            total_subjects = len(group_data)
            
            if total_subjects > 0:
                # è®¡ç®—å‘ç”Ÿä¸è‰¯äº‹ä»¶çš„å—è¯•è€…æ•°
                if df[ae_var].dtype in ['object', 'category']:
                    # åˆ†ç±»å˜é‡ - å‡è®¾éç©ºä¸”ä¸ä¸º"æ— "è¡¨ç¤ºå‘ç”Ÿ
                    ae_subjects = group_data[
                        (group_data[ae_var].notna()) & 
                        (group_data[ae_var] != 'æ— ') & 
                        (group_data[ae_var] != 'å¦')
                    ]
                else:
                    # æ•°å€¼å˜é‡ - å‡è®¾>0è¡¨ç¤ºå‘ç”Ÿ
                    ae_subjects = group_data[group_data[ae_var] > 0]
                
                ae_count = len(ae_subjects)
                ae_rate = ae_count / total_subjects * 100
                
                ae_summary.append({
                    'ä¸è‰¯äº‹ä»¶': ae_var,
                    'æ²»ç–—ç»„': group,
                    'æ€»ä¾‹æ•°': total_subjects,
                    'å‘ç”Ÿä¾‹æ•°': ae_count,
                    'å‘ç”Ÿç‡(%)': ae_rate
                })
    
    ae_summary_df = pd.DataFrame(ae_summary)
    
    if not ae_summary_df.empty:
        # æ˜¾ç¤ºæ±‡æ€»è¡¨
        pivot_table = ae_summary_df.pivot(index='ä¸è‰¯äº‹ä»¶', columns='æ²»ç–—ç»„', values='å‘ç”Ÿç‡(%)')
        st.dataframe(pivot_table.round(2), use_container_width=True)
        
        # ç»Ÿè®¡æ£€éªŒ
        st.markdown("##### ğŸ§® ç»„é—´æ¯”è¾ƒ")
        
        ae_comparison_results = []
        
        for ae_var in ae_vars:
            # åˆ›å»ºåˆ—è”è¡¨
            ae_crosstab_data = []
            
            for group in treatment_groups:
                group_data = df[df['æ²»ç–—ç»„'] == group]
                
                if df[ae_var].dtype in ['object', 'category']:
                    ae_count = len(group_data[
                        (group_data[ae_var].notna()) & 
                        (group_data[ae_var] != 'æ— ') & 
                        (group_data[ae_var] != 'å¦')
                    ])
                else:
                    ae_count = len(group_data[group_data[ae_var] > 0])
                
                no_ae_count = len(group_data) - ae_count
                
                ae_crosstab_data.extend([
                    {'æ²»ç–—ç»„': group, 'ä¸è‰¯äº‹ä»¶': 'æ˜¯', 'äººæ•°': ae_count},
                    {'æ²»ç–—ç»„': group, 'ä¸è‰¯äº‹ä»¶': 'å¦', 'äººæ•°': no_ae_count}
                ])
            
            ae_crosstab_df = pd.DataFrame(ae_crosstab_data)
            crosstab = pd.crosstab(ae_crosstab_df['ä¸è‰¯äº‹ä»¶'], ae_crosstab_df['æ²»ç–—ç»„'], 
                                 values=ae_crosstab_df['äººæ•°'], aggfunc='sum')
            
            # ç»Ÿè®¡æ£€éªŒ
            try:
                if len(treatment_groups) == 2 and crosstab.shape == (2, 2):
                    # Fisherç²¾ç¡®æ£€éªŒ
                    _, p_value = fisher_exact(crosstab.values)
                    test_method = "Fisherç²¾ç¡®æ£€éªŒ"
                    
                    # è®¡ç®—é£é™©æ¯”
                    group1_ae_rate = crosstab.iloc[1, 0] / crosstab.iloc[:, 0].sum()
                    group2_ae_rate = crosstab.iloc[1, 1] / crosstab.iloc[:, 1].sum()
                    risk_ratio = group1_ae_rate / group2_ae_rate if group2_ae_rate > 0 else float('inf')
                    
                else:
                    # å¡æ–¹æ£€éªŒ
                    chi2, p_value, _, _ = chi2_contingency(crosstab)
                    test_method = "å¡æ–¹æ£€éªŒ"
                    risk_ratio = None
                
                ae_comparison_results.append({
                    'ä¸è‰¯äº‹ä»¶': ae_var,
                    'æ£€éªŒæ–¹æ³•': test_method,
                    'På€¼': f"{p_value:.4f}",
                    'æ˜¾è‘—æ€§': "æ˜¯" if p_value < 0.05 else "å¦",
                    'é£é™©æ¯”': f"{risk_ratio:.3f}" if risk_ratio and risk_ratio != float('inf') else "N/A"
                })
                
            except Exception as e:
                ae_comparison_results.append({
                    'ä¸è‰¯äº‹ä»¶': ae_var,
                    'æ£€éªŒæ–¹æ³•': "è®¡ç®—å¤±è´¥",
                    'På€¼': "N/A",
                    'æ˜¾è‘—æ€§': "N/A",
                    'é£é™©æ¯”': "N/A"
                })
        
        comparison_df = pd.DataFrame(ae_comparison_results)
        st.dataframe(comparison_df, use_container_width=True)
        
        # å¯è§†åŒ–
        st.markdown("##### ğŸ“Š ä¸è‰¯äº‹ä»¶å¯è§†åŒ–")
        
        # é€‰æ‹©å¯è§†åŒ–çš„ä¸è‰¯äº‹ä»¶
        viz_ae = st.selectbox("é€‰æ‹©è¦å¯è§†åŒ–çš„ä¸è‰¯äº‹ä»¶", ae_vars)
        
        if viz_ae:
            # å‘ç”Ÿç‡æŸ±çŠ¶å›¾
            viz_data = ae_summary_df[ae_summary_df['ä¸è‰¯äº‹ä»¶'] == viz_ae]
            
            fig = px.bar(
                viz_data, x='æ²»ç–—ç»„', y='å‘ç”Ÿç‡(%)',
                title=f"{viz_ae} å„ç»„å‘ç”Ÿç‡æ¯”è¾ƒ",
                color='æ²»ç–—ç»„',
                text='å‘ç”Ÿç‡(%)'
            )
            fig.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)

def laboratory_analysis(df, safety_vars):
    """å®éªŒå®¤æ£€æŸ¥åˆ†æ"""
    st.markdown("#### ğŸ”¬ å®éªŒå®¤æ£€æŸ¥åˆ†æ")
    
    # è¯†åˆ«å®éªŒå®¤æ£€æŸ¥å˜é‡
    lab_keywords = ['è¡€å¸¸è§„', 'ç”ŸåŒ–', 'è‚åŠŸèƒ½', 'è‚¾åŠŸèƒ½', 'è¡€ç³–', 'è¡€è„‚', 'ALT', 'AST', 'è‚Œé…', 'å°¿ç´ ']
    lab_vars = [var for var in safety_vars if any(keyword in var for keyword in lab_keywords)]
    
    if not lab_vars:
        st.warning("âš ï¸ æœªè¯†åˆ«åˆ°å®éªŒå®¤æ£€æŸ¥å˜é‡")
        return
    
    # é€‰æ‹©å®éªŒå®¤æŒ‡æ ‡
    selected_lab_vars = st.multiselect(
        "é€‰æ‹©å®éªŒå®¤æ£€æŸ¥æŒ‡æ ‡",
        lab_vars,
        default=lab_vars[:5] if len(lab_vars) >= 5 else lab_vars
    )
    
    if not selected_lab_vars:
        return
    
    # åˆ†æç±»å‹
    analysis_type = st.radio(
        "åˆ†æç±»å‹",
        ["åŸºçº¿ä¸æ²»ç–—åæ¯”è¾ƒ", "å¼‚å¸¸å€¼åˆ†æ", "ä¸´åºŠæ˜¾è‘—æ€§å˜åŒ–"],
        horizontal=True
    )
    
    if analysis_type == "åŸºçº¿ä¸æ²»ç–—åæ¯”è¾ƒ":
        baseline_vs_treatment_analysis(df, selected_lab_vars)
    elif analysis_type == "å¼‚å¸¸å€¼åˆ†æ":
        lab_abnormal_analysis(df, selected_lab_vars)
    elif analysis_type == "ä¸´åºŠæ˜¾è‘—æ€§å˜åŒ–":
        clinically_significant_changes(df, selected_lab_vars)

def baseline_vs_treatment_analysis(df, lab_vars):
    """åŸºçº¿ä¸æ²»ç–—åæ¯”è¾ƒåˆ†æ"""
    st.markdown("##### ğŸ“Š åŸºçº¿ä¸æ²»ç–—åæ¯”è¾ƒ")
    
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    
    for lab_var in lab_vars:
        st.markdown(f"**{lab_var}:**")
        
        # å‡è®¾åŸºçº¿å’Œæ²»ç–—åæ•°æ®åœ¨åŒä¸€è¡Œ
        baseline_col = f"{lab_var}_åŸºçº¿"
        followup_col = f"{lab_var}_éšè®¿"
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„åŸºçº¿å’Œéšè®¿åˆ—ï¼Œè·³è¿‡
        if baseline_col not in df.columns or followup_col not in df.columns:
            st.info(f"æœªæ‰¾åˆ°{lab_var}çš„åŸºçº¿å’Œéšè®¿æ•°æ®åˆ—")
            continue
        
        comparison_results = []
        
        for group in treatment_groups:
            group_data = df[df['æ²»ç–—ç»„'] == group]
            
            baseline_data = group_data[baseline_col].dropna()
            followup_data = group_data[followup_col].dropna()
            
            if len(baseline_data) > 0 and len(followup_data) > 0:
                # é…å¯¹tæ£€éªŒæˆ–Wilcoxonç¬¦å·ç§©æ£€éªŒ
                paired_data = group_data[[baseline_col, followup_col]].dropna()
                
                if len(paired_data) > 0:
                    baseline_paired = paired_data[baseline_col]
                    followup_paired = paired_data[followup_col]
                    
                    # æ£€éªŒæ­£æ€æ€§
                    diff_data = followup_paired - baseline_paired
                    
                    if is_normally_distributed(diff_data):
                        # é…å¯¹tæ£€éªŒ
                        t_stat, p_value = stats.ttest_rel(followup_paired, baseline_paired)
                        test_method = "é…å¯¹tæ£€éªŒ"
                    else:
                        # Wilcoxonç¬¦å·ç§©æ£€éªŒ
                        w_stat, p_value = wilcoxon(followup_paired, baseline_paired)
                        test_method = "Wilcoxonç¬¦å·ç§©æ£€éªŒ"
                    
                    mean_change = followup_paired.mean() - baseline_paired.mean()
                    
                    comparison_results.append({
                        'æ²»ç–—ç»„': group,
                        'åŸºçº¿å‡å€¼': baseline_paired.mean(),
                        'éšè®¿å‡å€¼': followup_paired.mean(),
                        'å˜åŒ–é‡': mean_change,
                        'æ£€éªŒæ–¹æ³•': test_method,
                        'På€¼': f"{p_value:.4f}",
                        'æ˜¾è‘—æ€§': "æ˜¯" if p_value < 0.05 else "å¦"
                    })
        
        if comparison_results:
            results_df = pd.DataFrame(comparison_results)
            st.dataframe(results_df.round(3), use_container_width=True)

def subgroup_analysis(df):
    """äºšç»„åˆ†æ"""
    st.markdown("### ğŸ“‹ äºšç»„åˆ†æ")
    st.markdown("*æ¢ç´¢ä¸åŒäºšç»„ä¸­çš„æ²»ç–—æ•ˆæœ*")
    
    # é€‰æ‹©äºšç»„å˜é‡
    subgroup_vars = [col for col in df.columns 
                     if col not in ['å—è¯•è€…ID', 'æ²»ç–—ç»„'] 
                     and (df[col].dtype in ['object', 'category'] or df[col].nunique() <= 10)]
    
    if not subgroup_vars:
        st.warning("âš ï¸ æœªæ‰¾åˆ°é€‚åˆçš„äºšç»„å˜é‡")
        return
    
    # é€‰æ‹©ç»ˆç‚¹å˜é‡
    endpoint_vars = df.select_dtypes(include=[np.number]).columns.tolist()
    endpoint_vars = [col for col in endpoint_vars if col != 'å—è¯•è€…ID']
    
        # äºšç»„åˆ†æè®¾ç½®
    col1, col2 = st.columns(2)
    
    with col1:
        selected_subgroup = st.selectbox("é€‰æ‹©äºšç»„å˜é‡", subgroup_vars)
        selected_endpoint = st.selectbox("é€‰æ‹©ç»ˆç‚¹å˜é‡", endpoint_vars)
    
    with col2:
        interaction_test = st.checkbox("è¿›è¡Œäº¤äº’ä½œç”¨æ£€éªŒ", value=True)
        forest_plot = st.checkbox("ç”Ÿæˆæ£®æ—å›¾", value=True)
    
    if not selected_subgroup or not selected_endpoint:
        return
    
    # æ‰§è¡Œäºšç»„åˆ†æ
    perform_subgroup_analysis(df, selected_subgroup, selected_endpoint, interaction_test, forest_plot)

def perform_subgroup_analysis(df, subgroup_var, endpoint_var, interaction_test, forest_plot):
    """æ‰§è¡Œäºšç»„åˆ†æ"""
    st.markdown(f"#### ğŸ“Š {subgroup_var} äºšç»„ä¸­ {endpoint_var} çš„åˆ†æç»“æœ")
    
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    subgroups = df[subgroup_var].unique()
    
    # äºšç»„åˆ†æç»“æœ
    subgroup_results = []
    
    for subgroup in subgroups:
        subgroup_data = df[df[subgroup_var] == subgroup]
        
        if len(subgroup_data) < 10:  # æ ·æœ¬é‡å¤ªå°
            continue
        
        # è®¡ç®—å„æ²»ç–—ç»„åœ¨è¯¥äºšç»„ä¸­çš„ç»Ÿè®¡é‡
        group_stats = {}
        group_data_list = []
        
        for group in treatment_groups:
            group_subgroup_data = subgroup_data[subgroup_data['æ²»ç–—ç»„'] == group][endpoint_var].dropna()
            
            if len(group_subgroup_data) > 0:
                group_stats[group] = {
                    'n': len(group_subgroup_data),
                    'mean': group_subgroup_data.mean(),
                    'std': group_subgroup_data.std(),
                    'median': group_subgroup_data.median()
                }
                group_data_list.append(group_subgroup_data)
        
        # ç»Ÿè®¡æ£€éªŒ
        if len(group_data_list) >= 2:
            try:
                if len(treatment_groups) == 2:
                    # ä¸¤ç»„æ¯”è¾ƒ
                    group1_data, group2_data = group_data_list[0], group_data_list[1]
                    
                    if (is_normally_distributed(group1_data) and is_normally_distributed(group2_data) 
                        and len(group1_data) >= 8 and len(group2_data) >= 8):
                        # tæ£€éªŒ
                        t_stat, p_value = ttest_ind(group1_data, group2_data)
                        test_method = "tæ£€éªŒ"
                        
                        # æ•ˆåº”é‡
                        effect_size = calculate_cohens_d(group1_data, group2_data)
                        
                        # å‡å€¼å·®åŠç½®ä¿¡åŒºé—´
                        mean_diff = group1_data.mean() - group2_data.mean()
                        pooled_se = np.sqrt(group1_data.var()/len(group1_data) + group2_data.var()/len(group2_data))
                        
                        # è®¡ç®—è‡ªç”±åº¦
                        df_welch = (group1_data.var()/len(group1_data) + group2_data.var()/len(group2_data))**2 / (
                            (group1_data.var()/len(group1_data))**2/(len(group1_data)-1) + 
                            (group2_data.var()/len(group2_data))**2/(len(group2_data)-1)
                        )
                        
                        t_critical = stats.t.ppf(0.975, df_welch)
                        ci_lower = mean_diff - t_critical * pooled_se
                        ci_upper = mean_diff + t_critical * pooled_se
                        
                    else:
                        # Mann-Whitney Uæ£€éªŒ
                        u_stat, p_value = mannwhitneyu(group1_data, group2_data, alternative='two-sided')
                        test_method = "Mann-Whitney U"
                        
                        # æ•ˆåº”é‡
                        z_score = stats.norm.ppf(1 - p_value/2)
                        effect_size = abs(z_score) / np.sqrt(len(group1_data) + len(group2_data))
                        
                        # ä¸­ä½æ•°å·®å¼‚
                        mean_diff = group1_data.median() - group2_data.median()
                        ci_lower, ci_upper = None, None
                
                else:
                    # å¤šç»„æ¯”è¾ƒ
                    all_normal = all(is_normally_distributed(data) for data in group_data_list if len(data) >= 8)
                    
                    if all_normal:
                        f_stat, p_value = stats.f_oneway(*group_data_list)
                        test_method = "ANOVA"
                    else:
                        h_stat, p_value = stats.kruskal(*group_data_list)
                        test_method = "Kruskal-Wallis"
                    
                    effect_size = None
                    mean_diff = None
                    ci_lower, ci_upper = None, None
                
                # ä¿å­˜ç»“æœ
                result = {
                    'äºšç»„': f"{subgroup_var}={subgroup}",
                    'æ ·æœ¬é‡': sum(stats['n'] for stats in group_stats.values()),
                    'æ£€éªŒæ–¹æ³•': test_method,
                    'På€¼': p_value,
                    'æ˜¾è‘—æ€§': "æ˜¯" if p_value < 0.05 else "å¦"
                }
                
                # æ·»åŠ å„ç»„ç»Ÿè®¡é‡
                for group, stats_dict in group_stats.items():
                    result[f'{group}_n'] = stats_dict['n']
                    result[f'{group}_mean'] = stats_dict['mean']
                    result[f'{group}_std'] = stats_dict['std']
                
                if len(treatment_groups) == 2:
                    result['å‡å€¼å·®å¼‚'] = mean_diff
                    result['æ•ˆåº”é‡'] = effect_size
                    if ci_lower is not None:
                        result['95%CI_ä¸‹é™'] = ci_lower
                        result['95%CI_ä¸Šé™'] = ci_upper
                
                subgroup_results.append(result)
                
            except Exception as e:
                st.warning(f"äºšç»„ {subgroup} åˆ†æå¤±è´¥: {str(e)}")
    
    # æ˜¾ç¤ºäºšç»„åˆ†æç»“æœ
    if subgroup_results:
        results_df = pd.DataFrame(subgroup_results)
        st.dataframe(results_df.round(4), use_container_width=True)
        
        # äº¤äº’ä½œç”¨æ£€éªŒ
        if interaction_test and len(treatment_groups) == 2:
            perform_interaction_test(df, subgroup_var, endpoint_var)
        
        # æ£®æ—å›¾
        if forest_plot and len(treatment_groups) == 2:
            create_forest_plot(results_df, subgroup_var, endpoint_var)
        
        # äºšç»„å¯è§†åŒ–
        create_subgroup_visualization(df, subgroup_var, endpoint_var)
    
    else:
        st.warning("âš ï¸ æœªèƒ½å®Œæˆäºšç»„åˆ†æï¼Œå¯èƒ½æ˜¯æ ·æœ¬é‡ä¸è¶³")

def perform_interaction_test(df, subgroup_var, endpoint_var):
    """æ‰§è¡Œäº¤äº’ä½œç”¨æ£€éªŒ"""
    st.markdown("##### ğŸ”„ äº¤äº’ä½œç”¨æ£€éªŒ")
    
    try:
        import statsmodels.api as sm
        from statsmodels.formula.api import ols
        
        # æ„å»ºäº¤äº’ä½œç”¨æ¨¡å‹
        # éœ€è¦ç¡®ä¿åˆ†ç±»å˜é‡æ­£ç¡®ç¼–ç 
        df_model = df.copy()
        
        # å¦‚æœäºšç»„å˜é‡æ˜¯æ•°å€¼å‹ä½†å®é™…æ˜¯åˆ†ç±»å˜é‡ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if df_model[subgroup_var].dtype in [np.number] and df_model[subgroup_var].nunique() <= 10:
            df_model[subgroup_var] = df_model[subgroup_var].astype(str)
        
        formula = f"{endpoint_var} ~ C(æ²»ç–—ç»„) * C({subgroup_var})"
        
        model = ols(formula, data=df_model).fit()
        
        # æå–äº¤äº’ä½œç”¨é¡¹çš„På€¼
        interaction_params = [param for param in model.params.index if 'æ²»ç–—ç»„' in param and subgroup_var in param]
        
        if interaction_params:
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**äº¤äº’ä½œç”¨æ£€éªŒç»“æœ:**")
                for param in interaction_params:
                    coef = model.params[param]
                    p_value = model.pvalues[param]
                    st.write(f"â€¢ {param}")
                    st.write(f"  ç³»æ•°: {coef:.4f}")
                    st.write(f"  På€¼: {p_value:.4f}")
            
            with col2:
                # æ•´ä½“äº¤äº’ä½œç”¨æ£€éªŒ
                overall_p = min(model.pvalues[param] for param in interaction_params)
                
                if overall_p < 0.05:
                    st.success("âœ… å­˜åœ¨æ˜¾è‘—çš„æ²»ç–—Ã—äºšç»„äº¤äº’ä½œç”¨")
                    st.info("ğŸ’¡ ä¸åŒäºšç»„çš„æ²»ç–—æ•ˆæœå­˜åœ¨å·®å¼‚")
                else:
                    st.info("â„¹ï¸ æ— æ˜¾è‘—çš„æ²»ç–—Ã—äºšç»„äº¤äº’ä½œç”¨")
                    st.info("ğŸ’¡ å„äºšç»„çš„æ²»ç–—æ•ˆæœç›¸ä¼¼")
                
                # æ¨¡å‹æ‹Ÿåˆä¼˜åº¦
                st.write(f"RÂ²: {model.rsquared:.3f}")
                st.write(f"è°ƒæ•´RÂ²: {model.rsquared_adj:.3f}")
        
    except ImportError:
        st.warning("âš ï¸ éœ€è¦å®‰è£…statsmodelsåº“è¿›è¡Œäº¤äº’ä½œç”¨æ£€éªŒ")
    except Exception as e:
        st.error(f"âŒ äº¤äº’ä½œç”¨æ£€éªŒå¤±è´¥: {str(e)}")

def create_forest_plot(results_df, subgroup_var, endpoint_var):
    """åˆ›å»ºæ£®æ—å›¾"""
    st.markdown("##### ğŸŒ² æ£®æ—å›¾")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ•°æ®
    if 'å‡å€¼å·®å¼‚' not in results_df.columns:
        st.warning("âš ï¸ ç¼ºå°‘å‡å€¼å·®å¼‚æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆæ£®æ—å›¾")
        return
    
    # å‡†å¤‡æ£®æ—å›¾æ•°æ®
    forest_data = []
    
    for _, row in results_df.iterrows():
        if pd.notna(row['å‡å€¼å·®å¼‚']):
            forest_data.append({
                'äºšç»„': row['äºšç»„'],
                'å‡å€¼å·®å¼‚': row['å‡å€¼å·®å¼‚'],
                'ä¸‹é™': row.get('95%CI_ä¸‹é™', row['å‡å€¼å·®å¼‚'] - 1.96 * 0.5),  # ç®€åŒ–çš„ç½®ä¿¡åŒºé—´
                'ä¸Šé™': row.get('95%CI_ä¸Šé™', row['å‡å€¼å·®å¼‚'] + 1.96 * 0.5),
                'æ˜¾è‘—æ€§': row['æ˜¾è‘—æ€§']
            })
    
    if not forest_data:
        st.warning("âš ï¸ æ— æœ‰æ•ˆæ•°æ®ç”Ÿæˆæ£®æ—å›¾")
        return
    
    forest_df = pd.DataFrame(forest_data)
    
    # åˆ›å»ºæ£®æ—å›¾
    fig = go.Figure()
    
    # æ·»åŠ ç½®ä¿¡åŒºé—´
    for i, row in forest_df.iterrows():
        color = 'red' if row['æ˜¾è‘—æ€§'] == 'æ˜¯' else 'blue'
        
        # æ°´å¹³çº¿è¡¨ç¤ºç½®ä¿¡åŒºé—´
        fig.add_trace(go.Scatter(
            x=[row['ä¸‹é™'], row['ä¸Šé™']],
            y=[i, i],
            mode='lines',
            line=dict(color=color, width=2),
            showlegend=False
        ))
        
        # ç‚¹è¡¨ç¤ºå‡å€¼å·®å¼‚
        fig.add_trace(go.Scatter(
            x=[row['å‡å€¼å·®å¼‚']],
            y=[i],
            mode='markers',
            marker=dict(color=color, size=8, symbol='diamond'),
            name=row['äºšç»„'],
            showlegend=False
        ))
    
    # æ·»åŠ æ— æ•ˆçº¿ (x=0)
    fig.add_vline(x=0, line_dash="dash", line_color="gray")
    
    # æ›´æ–°å¸ƒå±€
    fig.update_layout(
        title=f"{subgroup_var} äºšç»„åˆ†ææ£®æ—å›¾",
        xaxis_title=f"{endpoint_var} å‡å€¼å·®å¼‚",
        yaxis_title="äºšç»„",
        yaxis=dict(
            tickmode='array',
            tickvals=list(range(len(forest_df))),
            ticktext=forest_df['äºšç»„'].tolist()
        ),
        height=max(400, len(forest_df) * 50),
        showlegend=False
    )
    
    st.plotly_chart(fig, use_container_width=True)

def create_subgroup_visualization(df, subgroup_var, endpoint_var):
    """åˆ›å»ºäºšç»„å¯è§†åŒ–"""
    st.markdown("##### ğŸ“Š äºšç»„å¯è§†åŒ–")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # ç®±çº¿å›¾
        fig_box = px.box(
            df, x=subgroup_var, y=endpoint_var, color='æ²»ç–—ç»„',
            title=f"{endpoint_var} åœ¨ä¸åŒ{subgroup_var}äºšç»„ä¸­çš„åˆ†å¸ƒ"
        )
        fig_box.update_layout(height=400)
        st.plotly_chart(fig_box, use_container_width=True)
    
    with col2:
        # å‡å€¼å›¾
        mean_data = df.groupby([subgroup_var, 'æ²»ç–—ç»„'])[endpoint_var].agg(['mean', 'std', 'count']).reset_index()
        
        fig_mean = px.bar(
            mean_data, x=subgroup_var, y='mean', color='æ²»ç–—ç»„',
            title=f"{endpoint_var} å„äºšç»„å‡å€¼æ¯”è¾ƒ",
            barmode='group',
            error_y='std'
        )
        fig_mean.update_layout(height=400)
        st.plotly_chart(fig_mean, use_container_width=True)

def time_trend_analysis(df):
    """æ—¶é—´è¶‹åŠ¿åˆ†æ"""
    st.markdown("### â±ï¸ æ—¶é—´è¶‹åŠ¿åˆ†æ")
    st.markdown("*åˆ†ææŒ‡æ ‡éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿*")
    
    # è¯†åˆ«æ—¶é—´å˜é‡
    time_vars = identify_time_variables(df)
    
    if not time_vars:
        st.warning("âš ï¸ æœªè¯†åˆ«åˆ°æ—¶é—´ç›¸å…³å˜é‡")
        return
    
    # é€‰æ‹©åˆ†æå˜é‡
    col1, col2 = st.columns(2)
    
    with col1:
        time_var = st.selectbox("é€‰æ‹©æ—¶é—´å˜é‡", time_vars)
        outcome_vars = df.select_dtypes(include=[np.number]).columns.tolist()
        outcome_vars = [col for col in outcome_vars if col not in ['å—è¯•è€…ID'] + time_vars]
        outcome_var = st.selectbox("é€‰æ‹©ç»“å±€å˜é‡", outcome_vars)
    
    with col2:
        analysis_type = st.selectbox(
            "åˆ†æç±»å‹",
            ["çº¿æ€§è¶‹åŠ¿åˆ†æ", "é‡å¤æµ‹é‡åˆ†æ", "ç”Ÿé•¿æ›²çº¿åˆ†æ", "æ—¶ç‚¹æ¯”è¾ƒåˆ†æ"]
        )
    
    if not time_var or not outcome_var:
        return
    
    if analysis_type == "çº¿æ€§è¶‹åŠ¿åˆ†æ":
        linear_trend_analysis(df, time_var, outcome_var)
    elif analysis_type == "é‡å¤æµ‹é‡åˆ†æ":
        repeated_measures_analysis(df, time_var, outcome_var)
    elif analysis_type == "ç”Ÿé•¿æ›²çº¿åˆ†æ":
        growth_curve_analysis(df, time_var, outcome_var)
    elif analysis_type == "æ—¶ç‚¹æ¯”è¾ƒåˆ†æ":
        timepoint_comparison_analysis(df, time_var, outcome_var)

def identify_time_variables(df):
    """è¯†åˆ«æ—¶é—´å˜é‡"""
    time_keywords = ['æ—¶é—´', 'å¤©æ•°', 'å‘¨æ•°', 'æœˆæ•°', 'è®¿é—®', 'éšè®¿', 'day', 'week', 'month', 'visit']
    
    time_vars = []
    
    for col in df.columns:
        if col in ['å—è¯•è€…ID', 'æ²»ç–—ç»„']:
            continue
        
        # æ£€æŸ¥åˆ—å
        if any(keyword in col.lower() for keyword in time_keywords):
            time_vars.append(col)
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        elif df[col].dtype in ['datetime64[ns]', 'timedelta64[ns]']:
            time_vars.append(col)
    
    return time_vars

def linear_trend_analysis(df, time_var, outcome_var):
    """çº¿æ€§è¶‹åŠ¿åˆ†æ"""
    st.markdown("#### ğŸ“ˆ çº¿æ€§è¶‹åŠ¿åˆ†æ")
    
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    
    # ä¸ºæ¯ä¸ªæ²»ç–—ç»„è¿›è¡Œçº¿æ€§å›å½’
    trend_results = []
    
    fig = go.Figure()
    
    for group in treatment_groups:
        group_data = df[df['æ²»ç–—ç»„'] == group][[time_var, outcome_var]].dropna()
        
        if len(group_data) < 3:
            continue
        
        x = group_data[time_var]
        y = group_data[outcome_var]
        
        # çº¿æ€§å›å½’
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        
        # é¢„æµ‹å€¼
        x_pred = np.linspace(x.min(), x.max(), 100)
        y_pred = slope * x_pred + intercept
        
        # æ·»åŠ æ•£ç‚¹å›¾
        fig.add_trace(go.Scatter(
            x=x, y=y,
            mode='markers',
            name=f'{group} (æ•°æ®ç‚¹)',
            marker=dict(size=6),
            showlegend=True
        ))
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        fig.add_trace(go.Scatter(
            x=x_pred, y=y_pred,
            mode='lines',
            name=f'{group} (è¶‹åŠ¿çº¿)',
            line=dict(dash='dash'),
            showlegend=True
        ))
        
        # ä¿å­˜ç»“æœ
        trend_results.append({
            'æ²»ç–—ç»„': group,
            'æ ·æœ¬é‡': len(group_data),
            'æ–œç‡': slope,
            'æˆªè·': intercept,
            'ç›¸å…³ç³»æ•°(r)': r_value,
            'RÂ²': r_value**2,
            'På€¼': p_value,
            'æ ‡å‡†è¯¯': std_err,
            'æ˜¾è‘—æ€§': "æ˜¯" if p_value < 0.05 else "å¦"
        })
    
    # æ›´æ–°å›¾è¡¨å¸ƒå±€
    fig.update_layout(
        title=f"{outcome_var} éš {time_var} çš„å˜åŒ–è¶‹åŠ¿",
        xaxis_title=time_var,
        yaxis_title=outcome_var,
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # æ˜¾ç¤ºè¶‹åŠ¿åˆ†æç»“æœ
    if trend_results:
        st.markdown("##### ğŸ“Š è¶‹åŠ¿åˆ†æç»“æœ")
        results_df = pd.DataFrame(trend_results)
        st.dataframe(results_df.round(4), use_container_width=True)
        
        # ç»„é—´è¶‹åŠ¿æ¯”è¾ƒ
        if len(trend_results) >= 2:
            st.markdown("##### ğŸ”„ ç»„é—´è¶‹åŠ¿æ¯”è¾ƒ")
            
            slopes = [result['æ–œç‡'] for result in trend_results]
            slope_diff = max(slopes) - min(slopes)
            
            st.write(f"â€¢ æœ€å¤§æ–œç‡å·®å¼‚: {slope_diff:.4f}")
            
            # ç®€å•çš„è¶‹åŠ¿æ¯”è¾ƒ
            positive_trends = [result['æ²»ç–—ç»„'] for result in trend_results if result['æ–œç‡'] > 0 and result['æ˜¾è‘—æ€§'] == 'æ˜¯']
            negative_trends = [result['æ²»ç–—ç»„'] for result in trend_results if result['æ–œç‡'] < 0 and result['æ˜¾è‘—æ€§'] == 'æ˜¯']
            
            if positive_trends:
                st.success(f"âœ… æ˜¾è‘—ä¸Šå‡è¶‹åŠ¿: {', '.join(positive_trends)}")
            if negative_trends:
                st.warning(f"âš ï¸ æ˜¾è‘—ä¸‹é™è¶‹åŠ¿: {', '.join(negative_trends)}")

def sensitivity_analysis(df):
    """æ•æ„Ÿæ€§åˆ†æ"""
    st.markdown("### ğŸ” æ•æ„Ÿæ€§åˆ†æ")
    st.markdown("*è¯„ä¼°åˆ†æç»“æœçš„ç¨³å¥æ€§*")
    
    # æ•æ„Ÿæ€§åˆ†æç±»å‹
    sensitivity_type = st.selectbox(
        "é€‰æ‹©æ•æ„Ÿæ€§åˆ†æç±»å‹",
        [
            "ç¼ºå¤±æ•°æ®å¤„ç†æ•æ„Ÿæ€§",
            "å¼‚å¸¸å€¼å½±å“åˆ†æ", 
            "åˆ†ææ–¹æ³•æ•æ„Ÿæ€§",
            "äºšç»„æ’é™¤æ•æ„Ÿæ€§",
            "åå˜é‡è°ƒæ•´æ•æ„Ÿæ€§"
        ]
    )
    
    # é€‰æ‹©ä¸»è¦åˆ†æå˜é‡
    endpoint_vars = df.select_dtypes(include=[np.number]).columns.tolist()
    endpoint_vars = [col for col in endpoint_vars if col != 'å—è¯•è€…ID']
    
    primary_endpoint = st.selectbox("é€‰æ‹©ä¸»è¦ç»ˆç‚¹", endpoint_vars)
    
    if not primary_endpoint:
        return
    
    if sensitivity_type == "ç¼ºå¤±æ•°æ®å¤„ç†æ•æ„Ÿæ€§":
        missing_data_sensitivity(df, primary_endpoint)
    elif sensitivity_type == "å¼‚å¸¸å€¼å½±å“åˆ†æ":
        outlier_influence_analysis(df, primary_endpoint)
    elif sensitivity_type == "åˆ†ææ–¹æ³•æ•æ„Ÿæ€§":
        analysis_method_sensitivity(df, primary_endpoint)
    elif sensitivity_type == "äºšç»„æ’é™¤æ•æ„Ÿæ€§":
        subgroup_exclusion_sensitivity(df, primary_endpoint)
    elif sensitivity_type == "åå˜é‡è°ƒæ•´æ•æ„Ÿæ€§":
        covariate_adjustment_sensitivity(df, primary_endpoint)

def missing_data_sensitivity(df, endpoint):
    """ç¼ºå¤±æ•°æ®å¤„ç†æ•æ„Ÿæ€§åˆ†æ"""
    st.markdown("#### ğŸ” ç¼ºå¤±æ•°æ®å¤„ç†æ•æ„Ÿæ€§åˆ†æ")
    
    # æ£€æŸ¥ç¼ºå¤±æƒ…å†µ
    missing_info = df[endpoint].isnull().sum()
    total_subjects = len(df)
    missing_rate = missing_info / total_subjects * 100
    
    st.info(f"ç¼ºå¤±æ•°æ®: {missing_info}/{total_subjects} ({missing_rate:.1f}%)")
    
    if missing_rate < 1:
        st.success("âœ… ç¼ºå¤±ç‡å¾ˆä½ï¼Œæ•æ„Ÿæ€§åˆ†æå¯èƒ½ä¸å¿…è¦")
        return
    
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    sensitivity_results = []
    
    # ä¸åŒçš„ç¼ºå¤±æ•°æ®å¤„ç†æ–¹æ³•
    methods = {
        "å®Œæ•´ç—…ä¾‹åˆ†æ": "complete_case",
        "æœ€åè§‚æµ‹å€¼ç»“è½¬(LOCF)": "locf", 
        "å‡å€¼æ’è¡¥": "mean_imputation",
        "æœ€åæƒ…å†µæ’è¡¥": "worst_case"
    }
    
    for method_name, method_code in methods.items():
        try:
            # æ ¹æ®æ–¹æ³•å¤„ç†æ•°æ®
            if method_code == "complete_case":
                analysis_df = df.dropna(subset=[endpoint])
            elif method_code == "locf":
                analysis_df = df.copy()
                # ç®€åŒ–çš„LOCF - ç”¨ç»„å†…å‡å€¼å¡«å……
                for group in treatment_groups:
                    group_mean = df[df['æ²»ç–—ç»„'] == group][endpoint].mean()
                    analysis_df.loc[
                        (analysis_df['æ²»ç–—ç»„'] == group) & (analysis_df[endpoint].isnull()), 
                        endpoint
                    ] = group_mean
            elif method_code == "mean_imputation":
                analysis_df = df.copy()
                overall_mean = df[endpoint].mean()
                analysis_df[endpoint].fillna(overall_mean, inplace=True)
            elif method_code == "worst_case":
                analysis_df = df.copy()
                # è¯•éªŒç»„ç”¨æœ€å°å€¼ï¼Œå¯¹ç…§ç»„ç”¨æœ€å¤§å€¼å¡«å……
                min_val = df[endpoint].min()
                max_val = df[endpoint].max()
                for group in treatment_groups:
                    if 'è¯•éªŒ' in group or 'æ²»ç–—' in group:
                        fill_val = min_val
                    else:
                        fill_val = max_val
                    analysis_df.loc[
                        (analysis_df['æ²»ç–—ç»„'] == group) & (analysis_df[endpoint].isnull()), 
                        endpoint
                    ] = fill_val
            
            # æ‰§è¡Œåˆ†æ
            if len(treatment_groups) == 2:
                group1_data = analysis_df[analysis_df['æ²»ç–—ç»„'] == treatment_groups[0]][endpoint]
                group2_data = analysis_df[analysis_df['æ²»ç–—ç»„'] == treatment_groups[1]][endpoint]
                
                # tæ£€éªŒ
                t_stat, p_value = ttest_ind(group1_data, group2_data)
                mean_diff = group1_data.mean() - group2_data.mean()
                
                sensitivity_results.append({
                    'å¤„ç†æ–¹æ³•': method_name,
                    'æ ·æœ¬é‡': len(analysis_df),
                    f'{treatment_groups[0]}_å‡å€¼': group1_data.mean(),
                    f'{treatment_groups[1]}_å‡å€¼': group2_data.mean(),
                    'å‡å€¼å·®å¼‚': mean_diff,
                    'tç»Ÿè®¡é‡': t_stat,
                    'På€¼': p_value,
                    'æ˜¾è‘—æ€§': "æ˜¯" if p_value < 0.05 else "å¦"
                })
        
        except Exception as e:
            sensitivity_results.append({
                'å¤„ç†æ–¹æ³•': method_name,
                'æ ·æœ¬é‡': 0,
                'ç»“æœ': f"åˆ†æå¤±è´¥: {str(e)}"
            })
    
    # æ˜¾ç¤ºç»“æœ
    if sensitivity_results:
        results_df = pd.DataFrame(sensitivity_results)
        st.dataframe(results_df.round(4), use_container_width=True)
        
        # ç»“æœä¸€è‡´æ€§è¯„ä¼°
        significant_methods = [result['å¤„ç†æ–¹æ³•'] for result in sensitivity_results 
                             if result.get('æ˜¾è‘—æ€§') == 'æ˜¯']
        
        st.markdown("##### ğŸ“Š æ•æ„Ÿæ€§åˆ†æç»“è®º")
        
        if len(significant_methods) == len(methods):
            st.success("âœ… æ‰€æœ‰ç¼ºå¤±æ•°æ®å¤„ç†æ–¹æ³•å‡æ˜¾ç¤ºæ˜¾è‘—å·®å¼‚ï¼Œç»“æœç¨³å¥")
        elif len(significant_methods) > len(methods) / 2:
            st.info(f"â„¹ï¸ å¤§éƒ¨åˆ†æ–¹æ³•æ˜¾ç¤ºæ˜¾è‘—å·®å¼‚ ({len(significant_methods)}/{len(methods)})")
        else:
            st.warning(f"âš ï¸ ç»“æœå¯¹ç¼ºå¤±æ•°æ®å¤„ç†æ–¹æ³•æ•æ„Ÿ ({len(significant_methods)}/{len(methods)} æ˜¾è‘—)")

def trial_summary_report(df):
    """è¯•éªŒæ€»ç»“æŠ¥å‘Š"""
    st.markdown("### ğŸ“„ è¯•éªŒæ€»ç»“æŠ¥å‘Š")
    st.markdown("*ç”Ÿæˆå®Œæ•´çš„ä¸´åºŠè¯•éªŒåˆ†ææŠ¥å‘Š*")
    
    # æŠ¥å‘Šè®¾ç½®
    with st.expander("ğŸ“‹ æŠ¥å‘Šè®¾ç½®", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            include_baseline = st.checkbox("åŒ…å«åŸºçº¿ç‰¹å¾", value=True)
            include_primary = st.checkbox("åŒ…å«ä¸»è¦ç»ˆç‚¹", value=True)
        
        with col2:
            include_secondary = st.checkbox("åŒ…å«æ¬¡è¦ç»ˆç‚¹", value=True)
            include_safety = st.checkbox("åŒ…å«å®‰å…¨æ€§åˆ†æ", value=True)
        
        with col3:
            include_subgroup = st.checkbox("åŒ…å«äºšç»„åˆ†æ", value=False)
            report_format = st.selectbox("æŠ¥å‘Šæ ¼å¼", ["HTML", "PDF", "Word"])
    
    if st.button("ğŸ”„ ç”ŸæˆæŠ¥å‘Š"):
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = generate_trial_report(
            df, include_baseline, include_primary, include_secondary, 
            include_safety, include_subgroup
        )
        
        # æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆ
        st.markdown("### ğŸ“– æŠ¥å‘Šé¢„è§ˆ")
        st.markdown(report_content, unsafe_allow_html=True)
        
        # æä¾›ä¸‹è½½
        if report_format == "HTML":
            st.download_button(
                label="ğŸ“¥ ä¸‹è½½HTMLæŠ¥å‘Š",
                data=report_content,
                file_name=f"ä¸´åºŠè¯•éªŒæŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                mime="text/html"
            )

def generate_trial_report(df, include_baseline, include_primary, include_secondary, include_safety, include_subgroup):
    """ç”Ÿæˆè¯•éªŒæŠ¥å‘Šå†…å®¹"""
    
    report_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>ä¸´åºŠè¯•éªŒåˆ†ææŠ¥å‘Š</title>
        <meta charset="utf-8">
        <style>
            body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
            .header {{ background-color: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 30px; }}
                        .section {{ margin-bottom: 30px; }}
            .section h2 {{ color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
            .section h3 {{ color: #34495e; margin-top: 25px; }}
            .table {{ border-collapse: collapse; width: 100%; margin: 15px 0; }}
            .table th, .table td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
            .table th {{ background-color: #f2f2f2; font-weight: bold; }}
            .highlight {{ background-color: #fff3cd; padding: 10px; border-radius: 5px; margin: 10px 0; }}
            .success {{ background-color: #d4edda; padding: 10px; border-radius: 5px; margin: 10px 0; }}
            .warning {{ background-color: #f8d7da; padding: 10px; border-radius: 5px; margin: 10px 0; }}
            .footer {{ margin-top: 50px; padding-top: 20px; border-top: 1px solid #ddd; font-size: 12px; color: #666; }}
        </style>
    </head>
    <body>
    
    <div class="header">
        <h1>ä¸´åºŠè¯•éªŒåˆ†ææŠ¥å‘Š</h1>
        <p><strong>ç”Ÿæˆæ—¶é—´:</strong> {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        <p><strong>æ•°æ®é›†:</strong> ä¸´åºŠè¯•éªŒæ•°æ®</p>
        <p><strong>åˆ†æè½¯ä»¶:</strong> ä¸´åºŠè¯•éªŒåˆ†æç³»ç»Ÿ</p>
    </div>
    """
    
    # è¯•éªŒæ¦‚å†µ
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    total_subjects = len(df)
    
    report_html += f"""
    <div class="section">
        <h2>1. è¯•éªŒæ¦‚å†µ</h2>
        <table class="table">
            <tr><th>é¡¹ç›®</th><th>æ•°å€¼</th></tr>
            <tr><td>æ€»å—è¯•è€…æ•°</td><td>{total_subjects}</td></tr>
            <tr><td>æ²»ç–—ç»„æ•°</td><td>{len(treatment_groups)}</td></tr>
            <tr><td>æ²»ç–—ç»„</td><td>{', '.join(treatment_groups)}</td></tr>
        </table>
        
        <h3>å—è¯•è€…åˆ†ç»„æƒ…å†µ</h3>
        <table class="table">
            <tr><th>æ²»ç–—ç»„</th><th>å—è¯•è€…æ•°</th><th>æ¯”ä¾‹(%)</th></tr>
    """
    
    for group in treatment_groups:
        group_count = len(df[df['æ²»ç–—ç»„'] == group])
        group_pct = group_count / total_subjects * 100
        report_html += f"<tr><td>{group}</td><td>{group_count}</td><td>{group_pct:.1f}</td></tr>"
    
    report_html += "</table></div>"
    
    # åŸºçº¿ç‰¹å¾åˆ†æ
    if include_baseline:
        baseline_vars = identify_baseline_variables(df)
        if baseline_vars:
            report_html += """
            <div class="section">
                <h2>2. åŸºçº¿ç‰¹å¾åˆ†æ</h2>
                <p>ä»¥ä¸‹æ˜¯å„æ²»ç–—ç»„åŸºçº¿ç‰¹å¾çš„æ¯”è¾ƒç»“æœï¼š</p>
            """
            
            # æ‰§è¡ŒåŸºçº¿åˆ†æ
            baseline_results = perform_baseline_analysis(df, baseline_vars[:10], True, 0.05, True)
            
            if baseline_results:
                report_html += '<table class="table"><tr><th>å˜é‡</th><th>ç±»å‹</th>'
                
                for group in treatment_groups:
                    report_html += f'<th>{group}</th>'
                
                report_html += '<th>På€¼</th><th>æ˜¾è‘—æ€§</th></tr>'
                
                for result in baseline_results:
                    report_html += f"""
                    <tr>
                        <td>{result.get('å˜é‡', 'N/A')}</td>
                        <td>{result.get('ç±»å‹', 'N/A')}</td>
                    """
                    
                    for group in treatment_groups:
                        group_stat = result.get(f'{group}', 'N/A')
                        report_html += f'<td>{group_stat}</td>'
                    
                    p_value = result.get('På€¼', 'N/A')
                    significance = result.get('æ˜¾è‘—æ€§', 'N/A')
                    
                    report_html += f'<td>{p_value}</td><td>{significance}</td></tr>'
                
                report_html += '</table>'
                
                # åŸºçº¿å¹³è¡¡æ€§è¯„ä¼°
                imbalanced_vars = detect_baseline_imbalance(baseline_results, 0.05)
                if imbalanced_vars:
                    report_html += f"""
                    <div class="warning">
                        <strong>åŸºçº¿ä¸å¹³è¡¡å˜é‡:</strong> {', '.join(imbalanced_vars)}
                        <br>å»ºè®®åœ¨ä¸»è¦åˆ†æä¸­è€ƒè™‘è¿™äº›å˜é‡ä½œä¸ºåå˜é‡è¿›è¡Œè°ƒæ•´ã€‚
                    </div>
                    """
                else:
                    report_html += """
                    <div class="success">
                        <strong>åŸºçº¿å¹³è¡¡æ€§è‰¯å¥½:</strong> æ‰€æœ‰åŸºçº¿å˜é‡åœ¨å„æ²»ç–—ç»„é—´å‡è¡¡è‰¯å¥½ã€‚
                    </div>
                    """
            
            report_html += "</div>"
    
    # ä¸»è¦ç»ˆç‚¹åˆ†æ
    if include_primary:
        endpoint_vars = identify_endpoint_variables(df, 'primary')
        if endpoint_vars:
            primary_endpoint = endpoint_vars[0]  # é€‰æ‹©ç¬¬ä¸€ä¸ªä½œä¸ºä¸»è¦ç»ˆç‚¹
            
            report_html += f"""
            <div class="section">
                <h2>3. ä¸»è¦ç»ˆç‚¹åˆ†æ</h2>
                <p><strong>ä¸»è¦ç»ˆç‚¹:</strong> {primary_endpoint}</p>
            """
            
            # æè¿°æ€§ç»Ÿè®¡
            desc_stats = []
            for group in treatment_groups:
                group_data = df[df['æ²»ç–—ç»„'] == group][primary_endpoint].dropna()
                if len(group_data) > 0:
                    desc_stats.append({
                        'æ²»ç–—ç»„': group,
                        'ä¾‹æ•°': len(group_data),
                        'å‡å€¼': group_data.mean(),
                        'æ ‡å‡†å·®': group_data.std(),
                        'ä¸­ä½æ•°': group_data.median()
                    })
            
            if desc_stats:
                report_html += '<h3>æè¿°æ€§ç»Ÿè®¡</h3><table class="table">'
                report_html += '<tr><th>æ²»ç–—ç»„</th><th>ä¾‹æ•°</th><th>å‡å€¼</th><th>æ ‡å‡†å·®</th><th>ä¸­ä½æ•°</th></tr>'
                
                for stat in desc_stats:
                    report_html += f"""
                    <tr>
                        <td>{stat['æ²»ç–—ç»„']}</td>
                        <td>{stat['ä¾‹æ•°']}</td>
                        <td>{stat['å‡å€¼']:.3f}</td>
                        <td>{stat['æ ‡å‡†å·®']:.3f}</td>
                        <td>{stat['ä¸­ä½æ•°']:.3f}</td>
                    </tr>
                    """
                
                report_html += '</table>'
                
                # ç»Ÿè®¡æ£€éªŒ
                if len(treatment_groups) == 2:
                    group1_data = df[df['æ²»ç–—ç»„'] == treatment_groups[0]][primary_endpoint].dropna()
                    group2_data = df[df['æ²»ç–—ç»„'] == treatment_groups[1]][primary_endpoint].dropna()
                    
                    if len(group1_data) > 0 and len(group2_data) > 0:
                        try:
                            t_stat, p_value = ttest_ind(group1_data, group2_data)
                            mean_diff = group1_data.mean() - group2_data.mean()
                            cohens_d = calculate_cohens_d(group1_data, group2_data)
                            
                            report_html += f"""
                            <h3>ç»Ÿè®¡æ£€éªŒç»“æœ</h3>
                            <table class="table">
                                <tr><th>æ£€éªŒé¡¹ç›®</th><th>ç»“æœ</th></tr>
                                <tr><td>æ£€éªŒæ–¹æ³•</td><td>ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ</td></tr>
                                <tr><td>tç»Ÿè®¡é‡</td><td>{t_stat:.4f}</td></tr>
                                <tr><td>På€¼</td><td>{p_value:.4f}</td></tr>
                                <tr><td>å‡å€¼å·®å¼‚</td><td>{mean_diff:.3f}</td></tr>
                                <tr><td>Cohen's d</td><td>{cohens_d:.3f}</td></tr>
                            </table>
                            """
                            
                            if p_value < 0.05:
                                report_html += """
                                <div class="success">
                                    <strong>ç»“è®º:</strong> ä¸¤ç»„é—´å·®å¼‚å…·æœ‰ç»Ÿè®¡å­¦æ„ä¹‰ (P < 0.05)ã€‚
                                </div>
                                """
                            else:
                                report_html += """
                                <div class="highlight">
                                    <strong>ç»“è®º:</strong> ä¸¤ç»„é—´å·®å¼‚æ— ç»Ÿè®¡å­¦æ„ä¹‰ (P â‰¥ 0.05)ã€‚
                                </div>
                                """
                        
                        except Exception as e:
                            report_html += f"<p>ç»Ÿè®¡æ£€éªŒå¤±è´¥: {str(e)}</p>"
            
            report_html += "</div>"
    
    # æ¬¡è¦ç»ˆç‚¹åˆ†æ
    if include_secondary:
        secondary_vars = identify_endpoint_variables(df, 'secondary')
        if secondary_vars:
            report_html += """
            <div class="section">
                <h2>4. æ¬¡è¦ç»ˆç‚¹åˆ†æ</h2>
                <p>æ¬¡è¦ç»ˆç‚¹åˆ†æç»“æœå¦‚ä¸‹ï¼š</p>
            """
            
            # åˆ†æå‰5ä¸ªæ¬¡è¦ç»ˆç‚¹
            for i, endpoint in enumerate(secondary_vars[:5], 1):
                try:
                    result = analyze_single_secondary_endpoint(df, endpoint, 0.05, True)
                    
                    report_html += f"""
                    <h3>4.{i} {endpoint}</h3>
                    <table class="table">
                        <tr><th>é¡¹ç›®</th><th>ç»“æœ</th></tr>
                        <tr><td>åˆ†æç±»å‹</td><td>{result.get('åˆ†æç±»å‹', 'N/A')}</td></tr>
                        <tr><td>æ£€éªŒæ–¹æ³•</td><td>{result.get('æ£€éªŒæ–¹æ³•', 'N/A')}</td></tr>
                        <tr><td>På€¼</td><td>{result.get('På€¼', 'N/A')}</td></tr>
                        <tr><td>æ˜¾è‘—æ€§</td><td>{result.get('æ˜¾è‘—æ€§', 'N/A')}</td></tr>
                    </table>
                    """
                    
                except Exception as e:
                    report_html += f"<p>{endpoint}: åˆ†æå¤±è´¥</p>"
            
            report_html += "</div>"
    
    # å®‰å…¨æ€§åˆ†æ
    if include_safety:
        safety_vars = identify_safety_variables(df)
        if safety_vars:
            report_html += """
            <div class="section">
                <h2>5. å®‰å…¨æ€§åˆ†æ</h2>
                <p>å®‰å…¨æ€§åˆ†æä¸»è¦å…³æ³¨ä¸è‰¯äº‹ä»¶çš„å‘ç”Ÿæƒ…å†µï¼š</p>
            """
            
            # ç®€åŒ–çš„å®‰å…¨æ€§åˆ†æ
            ae_summary = []
            
            for ae_var in safety_vars[:5]:  # åˆ†æå‰5ä¸ªå®‰å…¨æ€§å˜é‡
                for group in treatment_groups:
                    group_data = df[df['æ²»ç–—ç»„'] == group]
                    total_subjects = len(group_data)
                    
                    if total_subjects > 0:
                        # ç®€å•è®¡ç®—ä¸è‰¯äº‹ä»¶å‘ç”Ÿç‡
                        if df[ae_var].dtype in ['object', 'category']:
                            ae_count = len(group_data[
                                (group_data[ae_var].notna()) & 
                                (group_data[ae_var] != 'æ— ') & 
                                (group_data[ae_var] != 'å¦')
                            ])
                        else:
                            ae_count = len(group_data[group_data[ae_var] > 0])
                        
                        ae_rate = ae_count / total_subjects * 100
                        
                        ae_summary.append({
                            'å®‰å…¨æ€§æŒ‡æ ‡': ae_var,
                            'æ²»ç–—ç»„': group,
                            'æ€»ä¾‹æ•°': total_subjects,
                            'å‘ç”Ÿä¾‹æ•°': ae_count,
                            'å‘ç”Ÿç‡(%)': ae_rate
                        })
            
            if ae_summary:
                report_html += '<table class="table">'
                report_html += '<tr><th>å®‰å…¨æ€§æŒ‡æ ‡</th><th>æ²»ç–—ç»„</th><th>æ€»ä¾‹æ•°</th><th>å‘ç”Ÿä¾‹æ•°</th><th>å‘ç”Ÿç‡(%)</th></tr>'
                
                for ae in ae_summary:
                    report_html += f"""
                    <tr>
                        <td>{ae['å®‰å…¨æ€§æŒ‡æ ‡']}</td>
                        <td>{ae['æ²»ç–—ç»„']}</td>
                        <td>{ae['æ€»ä¾‹æ•°']}</td>
                        <td>{ae['å‘ç”Ÿä¾‹æ•°']}</td>
                        <td>{ae['å‘ç”Ÿç‡(%)']:.1f}</td>
                    </tr>
                    """
                
                report_html += '</table>'
                
                # å®‰å…¨æ€§æ€»ç»“
                total_ae_events = sum(ae['å‘ç”Ÿä¾‹æ•°'] for ae in ae_summary)
                if total_ae_events == 0:
                    report_html += """
                    <div class="success">
                        <strong>å®‰å…¨æ€§æ€»ç»“:</strong> è¯•éªŒæœŸé—´æœªè§‚å¯Ÿåˆ°æ˜æ˜¾çš„å®‰å…¨æ€§é—®é¢˜ã€‚
                    </div>
                    """
                else:
                    report_html += f"""
                    <div class="highlight">
                        <strong>å®‰å…¨æ€§æ€»ç»“:</strong> è¯•éªŒæœŸé—´å…±è§‚å¯Ÿåˆ° {total_ae_events} ä¾‹ä¸è‰¯äº‹ä»¶ï¼Œ
                        å„æ²»ç–—ç»„é—´çš„å®‰å…¨æ€§è¡¨ç°éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°ã€‚
                    </div>
                    """
            
            report_html += "</div>"
    
    # æ€»ç»“å’Œç»“è®º
    report_html += """
    <div class="section">
        <h2>6. æ€»ç»“å’Œç»“è®º</h2>
        <h3>ä¸»è¦å‘ç°</h3>
        <ul>
    """
    
    # åŸºäºåˆ†æç»“æœç”Ÿæˆç»“è®º
    if include_primary:
        endpoint_vars = identify_endpoint_variables(df, 'primary')
        if endpoint_vars and len(treatment_groups) == 2:
            primary_endpoint = endpoint_vars[0]
            try:
                group1_data = df[df['æ²»ç–—ç»„'] == treatment_groups[0]][primary_endpoint].dropna()
                group2_data = df[df['æ²»ç–—ç»„'] == treatment_groups[1]][primary_endpoint].dropna()
                
                if len(group1_data) > 0 and len(group2_data) > 0:
                    _, p_value = ttest_ind(group1_data, group2_data)
                    
                    if p_value < 0.05:
                        report_html += f"<li>ä¸»è¦ç»ˆç‚¹ {primary_endpoint} åœ¨ä¸¤æ²»ç–—ç»„é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ (P = {p_value:.4f})</li>"
                    else:
                        report_html += f"<li>ä¸»è¦ç»ˆç‚¹ {primary_endpoint} åœ¨ä¸¤æ²»ç–—ç»„é—´æ— æ˜¾è‘—å·®å¼‚ (P = {p_value:.4f})</li>"
            except:
                pass
    
    if include_baseline:
        baseline_vars = identify_baseline_variables(df)
        if baseline_vars:
            baseline_results = perform_baseline_analysis(df, baseline_vars[:10], True, 0.05, True)
            imbalanced_vars = detect_baseline_imbalance(baseline_results, 0.05)
            
            if imbalanced_vars:
                report_html += f"<li>å‘ç° {len(imbalanced_vars)} ä¸ªåŸºçº¿ä¸å¹³è¡¡å˜é‡ï¼Œå¯èƒ½å½±å“ç»“æœè§£é‡Š</li>"
            else:
                report_html += "<li>å„æ²»ç–—ç»„åŸºçº¿ç‰¹å¾å‡è¡¡è‰¯å¥½ï¼Œæ”¯æŒéšæœºåŒ–çš„æœ‰æ•ˆæ€§</li>"
    
    if include_safety:
        safety_vars = identify_safety_variables(df)
        if safety_vars:
            report_html += "<li>å®‰å…¨æ€§åˆ†ææ˜¾ç¤ºè¯•éªŒè¯ç‰©å…·æœ‰å¯æ¥å—çš„å®‰å…¨æ€§ç‰¹å¾</li>"
    
    report_html += """
        </ul>
        
        <h3>ç ”ç©¶å±€é™æ€§</h3>
        <ul>
            <li>æœ¬åˆ†æåŸºäºç°æœ‰æ•°æ®ï¼Œç»“æœè§£é‡Šéœ€ç»“åˆä¸´åºŠèƒŒæ™¯</li>
            <li>éƒ¨åˆ†åˆ†æå¯èƒ½å—åˆ°æ ·æœ¬é‡é™åˆ¶</li>
            <li>ç¼ºå¤±æ•°æ®çš„å¤„ç†å¯èƒ½å½±å“ç»“æœçš„ç¨³å¥æ€§</li>
        </ul>
        
        <h3>å»ºè®®</h3>
        <ul>
            <li>å»ºè®®ç»“åˆä¸´åºŠä¸“ä¸šçŸ¥è¯†è§£é‡Šç»Ÿè®¡ç»“æœ</li>
            <li>å¯¹äºæ˜¾è‘—æ€§ç»“æœï¼Œå»ºè®®è¯„ä¼°å…¶ä¸´åºŠæ„ä¹‰</li>
            <li>å»ºè®®è¿›è¡Œæ•æ„Ÿæ€§åˆ†æä»¥éªŒè¯ç»“æœçš„ç¨³å¥æ€§</li>
        </ul>
    </div>
    """
    
    # æŠ¥å‘Šç»“å°¾
    report_html += f"""
    <div class="footer">
        <p>æœ¬æŠ¥å‘Šç”±ä¸´åºŠè¯•éªŒåˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼Œç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}</p>
        <p>æ³¨æ„: æœ¬æŠ¥å‘Šä»…ä¾›ç»Ÿè®¡åˆ†æå‚è€ƒï¼Œä¸´åºŠå†³ç­–éœ€ç»“åˆä¸“ä¸šåŒ»å­¦åˆ¤æ–­ã€‚</p>
    </div>
    
    </body>
    </html>
    """
    
    return report_html

# è¾…åŠ©å‡½æ•°
def perform_cox_regression(df, time_col, event_col, adjustment_vars):
    """æ‰§è¡ŒCoxæ¯”ä¾‹é£é™©å›å½’"""
    try:
        from lifelines import CoxPHFitter
        
        # å‡†å¤‡æ•°æ®
        cox_data = df[['æ²»ç–—ç»„', time_col, event_col] + adjustment_vars].dropna()
        
        # ç¼–ç æ²»ç–—ç»„
        cox_data = pd.get_dummies(cox_data, columns=['æ²»ç–—ç»„'], prefix='treatment')
        
        # æ‹ŸåˆCoxæ¨¡å‹
        cph = CoxPHFitter()
        cph.fit(cox_data, duration_col=time_col, event_col=event_col)
        
        # æ˜¾ç¤ºç»“æœ
        st.markdown("**Coxå›å½’ç»“æœ:**")
        
        # æå–æ²»ç–—ç»„æ•ˆåº”
        treatment_params = [col for col in cph.params.index if 'treatment_' in col]
        
        for param in treatment_params:
            hr = np.exp(cph.params[param])
            ci_lower = np.exp(cph.confidence_intervals_.loc[param, 'coef lower 95%'])
            ci_upper = np.exp(cph.confidence_intervals_.loc[param, 'coef upper 95%'])
            p_value = cph.summary.loc[param, 'p']
            
            st.write(f"â€¢ {param}: HR = {hr:.3f} (95% CI: {ci_lower:.3f} - {ci_upper:.3f}), P = {p_value:.4f}")
        
        # æ¨¡å‹æ‹Ÿåˆåº¦
        st.write(f"â€¢ Concordance Index: {cph.concordance_index_:.3f}")
        st.write(f"â€¢ Log-likelihood: {cph.log_likelihood_:.2f}")
        
    except ImportError:
        st.warning("âš ï¸ éœ€è¦å®‰è£…lifelinesåº“è¿›è¡ŒCoxå›å½’åˆ†æ")
    except Exception as e:
        st.error(f"âŒ Coxå›å½’åˆ†æå¤±è´¥: {str(e)}")

def analyze_ordinal_endpoint(df, endpoint, alpha_level, confidence_level, adjustment_vars):
    """åˆ†ææœ‰åºåˆ†ç±»ç»ˆç‚¹"""
    st.markdown("#### ğŸ“Š æœ‰åºåˆ†ç±»ç»ˆç‚¹åˆ†æç»“æœ")
    
    treatment_groups = df['æ²»ç–—ç»„'].unique()
    
    # æè¿°æ€§ç»Ÿè®¡
    st.markdown("##### ğŸ“‹ æè¿°æ€§ç»Ÿè®¡")
    
    ordinal_stats = []
    for group in treatment_groups:
        group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
        
        if len(group_data) > 0:
            value_counts = group_data.value_counts().sort_index()
            total = len(group_data)
            
            # è®¡ç®—ç´¯ç§¯æ¯”ä¾‹
            cumulative_props = value_counts.cumsum() / total
            
            ordinal_stats.append({
                'æ²»ç–—ç»„': group,
                'æ€»ä¾‹æ•°': total,
                'åˆ†å¸ƒ': '; '.join([f"{val}:{count}({count/total*100:.1f}%)" 
                                for val, count in value_counts.items()]),
                'ä¸­ä½æ•°': group_data.median(),
                'ä¼—æ•°': group_data.mode().iloc[0] if not group_data.mode().empty else 'N/A'
            })
    
    ordinal_df = pd.DataFrame(ordinal_stats)
    st.dataframe(ordinal_df, use_container_width=True)
    
    # ç»Ÿè®¡æ£€éªŒ - Mann-Whitney U æˆ– Kruskal-Wallis
    st.markdown("##### ğŸ§® ç»Ÿè®¡æ£€éªŒ")
    
    if len(treatment_groups) == 2:
        group1_data = df[df['æ²»ç–—ç»„'] == treatment_groups[0]][endpoint].dropna()
        group2_data = df[df['æ²»ç–—ç»„'] == treatment_groups[1]][endpoint].dropna()
        
        if len(group1_data) > 0 and len(group2_data) > 0:
            u_stat, p_value = mannwhitneyu(group1_data, group2_data, alternative='two-sided')
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Mann-Whitney Uæ£€éªŒ:**")
                st.write(f"â€¢ Uç»Ÿè®¡é‡: {u_stat:.4f}")
                st.write(f"â€¢ På€¼: {p_value:.4f}")
                
                # æ•ˆåº”é‡
                z_score = stats.norm.ppf(1 - p_value/2)
                effect_size_r = abs(z_score) / np.sqrt(len(group1_data) + len(group2_data))
                st.write(f"â€¢ æ•ˆåº”é‡(r): {effect_size_r:.3f}")
            
            with col2:
                if p_value < alpha_level:
                    st.success(f"âœ… åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œä¸¤ç»„åˆ†å¸ƒå­˜åœ¨æ˜¾è‘—å·®å¼‚")
                else:
                    st.info(f"â„¹ï¸ åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œä¸¤ç»„åˆ†å¸ƒæ— æ˜¾è‘—å·®å¼‚")
    
    else:
        # å¤šç»„æ¯”è¾ƒ
        group_data_list = []
        for group in treatment_groups:
            group_data = df[df['æ²»ç–—ç»„'] == group][endpoint].dropna()
            group_data_list.append(group_data)
        
        if all(len(data) > 0 for data in group_data_list):
            h_stat, p_value = stats.kruskal(*group_data_list)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Kruskal-Wallisæ£€éªŒ:**")
                st.write(f"â€¢ Hç»Ÿè®¡é‡: {h_stat:.4f}")
                st.write(f"â€¢ På€¼: {p_value:.4f}")
                st.write(f"â€¢ è‡ªç”±åº¦: {len(treatment_groups)-1}")
            
            with col2:
                if p_value < alpha_level:
                    st.success(f"âœ… åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œå„ç»„åˆ†å¸ƒå­˜åœ¨æ˜¾è‘—å·®å¼‚")
                else:
                    st.info(f"â„¹ï¸ åœ¨Î±={alpha_level}æ°´å¹³ä¸‹ï¼Œå„ç»„åˆ†å¸ƒæ— æ˜¾è‘—å·®å¼‚")

# å…¶ä»–ç¼ºå¤±çš„è¾…åŠ©å‡½æ•°å¯ä»¥æ ¹æ®éœ€è¦ç»§ç»­æ·»åŠ ...

if __name__ == "__main__":
    clinical_trial_analysis()



            
